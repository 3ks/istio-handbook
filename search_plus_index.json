{"./":{"url":"./","title":"序","keywords":"","body":"Istio Handbook——Istio 服务网格进阶实战 Istio Service Mesh Advanced Practical - Master the Services in Post Kubernetes Era Istio 是由 Google、IBM、Lyft 等共同开源的 Service Mesh（服务网格）框架，于2017年初开始进入大众视野，作为云原生时代下承 Kubernetes、上接 Serverless 架构的重要基础设施层，地位至关重要。ServiceMesher 社区作为中国最早的一批在研究和推广 Service Mesh 技术的开源社区决定整合社区资源，合作撰写一本开源电子书作为服务网格智库。 您可以通过以下地址阅读或参与本书： GitHub 地址：https://github.com/servicemesher/istio-handbook Gitbook 在线浏览：http://www.servicemesher.com/istio-handbook/（注意最后需要加个/） 关于本书 本书起源于 rootsongjc/istio-handbook 及 ServiceMesher 社区创作的 Istio 知识图谱，集结社区力量合作创作而成。 本书基于 Istio 1.0+ 版本编写，包含但不限于 Istio 知识图谱中的主题。 版权 本书概念图，封面图片上海静安寺夜景，Jimmy Song 摄。 本书发行版权归属于电子工业出版社博文视点，未经授权请勿私自印刷发行。 参与本书 请参阅本书写作规范，加入 ServiceMesher 后进入 Slack channel 讨论。 ServiceMesher 社区 参与本书创作、讨论，获取关于服务网格的资讯、技术干货请加入我们的社区。 社区网站：http://www.servicemesher.com 微信公众号：ServiceMesher 点击关注【ServiceMesher】 微信公众号回复【加群】加入社区Copyright © servicemesher.com 2018-2019 all right reserved，powered by Gitbook Updated at 2019-03-13 13:41:15 "},"concepts-and-principle/":{"url":"concepts-and-principle/","title":"Istio 概念原理","summary":"本文是概念原理章节的开篇。","keywords":"","body":"概念原理 本章主要介绍 Istio 和 Service Mesh（服务网格）的概念和原理。 点击关注【ServiceMesher】 微信公众号回复【加群】加入社区Copyright © servicemesher.com 2018-2019 all right reserved，powered by Gitbook Updated at 2019-03-10 16:32:37 "},"concepts-and-principle/what-is-service-mesh.html":{"url":"concepts-and-principle/what-is-service-mesh.html","title":"什么是服务网格？","summary":"本文介绍了服务网格的定义和起源。","keywords":"","body":"什么是服务网格？ Service mesh 又译作 “服务网格”，作为服务间通信的基础设施层。Buoyant 公司的 CEO Willian Morgan 在他的这篇文章 WHAT’S A SERVICE MESH? AND WHY DO I NEED ONE? 中解释了什么是 Service Mesh，为什么云原生应用需要 Service Mesh。 服务网格是用于处理服务间通信的专用基础设施层。它负责通过包含现代云原生应用程序的复杂服务拓扑来可靠地传递请求。实际上，服务网格通常通过一组轻量级网络代理来实现，这些代理与应用程序代码一起部署，而不需要感知应用程序本身。—— Willian Morgan Buoyant CEO 服务网格（Service Mesh）这个术语通常用于描述构成这些应用程序的微服务网络以及应用之间的交互。随着规模和复杂性的增长，服务网格越来越难以理解和管理。它的需求包括服务发现、负载均衡、故障恢复、指标收集和监控以及通常更加复杂的运维需求，例如 A/B 测试、金丝雀发布、限流、访问控制和端到端认证等。 服务网格的特点 服务网格有如下几个特点： 应用程序间通讯的中间层 轻量级网络代理 应用程序无感知 解耦应用程序的重试/超时、监控、追踪和服务发现 目前两款流行的服务网格开源软件 Linkerd 和 Istio 都可以直接在 Kubernetes 中集成，其中 Linkerd 已经成为 CNCF 成员，Istio 在 2018年7月31日宣布 1.0。 理解服务网格 如果用一句话来解释什么是服务网格，可以将它比作是应用程序或者说微服务间的 TCP/IP，负责服务之间的网络调用、限流、熔断和监控。对于编写应用程序来说一般无须关心 TCP/IP 这一层（比如通过 HTTP 协议的 RESTful 应用），同样使用服务网格也就无须关心服务之间的那些原来是通过应用程序或者其他框架实现的事情，比如 Spring Cloud、OSS，现在只要交给服务网格就可以了。 Phil Calçado 在他的这篇博客 Pattern: Service Mesh 中详细解释了服务网格的来龙去脉： 从最原始的主机之间直接使用网线相连 网络层的出现 集成到应用程序内部的控制流 分解到应用程序外部的控制流 应用程序的中集成服务发现和断路器 出现了专门用于服务发现和断路器的软件包/库，如 Twitter 的 Finagle 和 Facebook 的 Proxygen，这时候还是集成在应用程序内部 出现了专门用于服务发现和断路器的开源软件，如 Netflix OSS、Airbnb 的 synapse 和 nerve 最后作为微服务的中间层服务网格出现 服务网格的架构如下图所示： 图片 - Service Mesh 架构图 图片来自：Pattern: Service Mesh 服务网格作为 sidecar 运行，对应用程序来说是透明，所有应用程序间的流量都会通过它，所以对应用程序流量的控制都可以在 serivce mesh 中实现。 服务网格如何工作？ 下面以 Istio 为例讲解服务网格如何在 Kubernetes 中工作。 Istio 将服务请求路由到目的地址，根据其中的参数判断是到生产环境、测试环境还是 staging 环境中的服务（服务可能同时部署在这三个环境中），是路由到本地环境还是公有云环境？所有的这些路由信息可以动态配置，可以是全局配置也可以为某些服务单独配置。 当 Istio 确认了目的地址后，将流量发送到相应服务发现端点，在 Kubernetes 中是 service，然后 service 会将服务转发给后端的实例。 Istio 根据它观测到最近请求的延迟时间，选择出所有应用程序的实例中响应最快的实例。 Istio 将请求发送给该实例，同时记录响应类型和延迟数据。 如果该实例挂了、不响应了或者进程不工作了，Istio 将把请求发送到其他实例上重试。 如果该实例持续返回 error，Istio 会将该实例从负载均衡池中移除，稍后再周期性得重试。 如果请求的截止时间已过，Istio 主动失败该请求，而不是再次尝试添加负载。 Istio 以 metric 和分布式追踪的形式捕获上述行为的各个方面，这些追踪信息将发送到集中 metric 系统。 为何使用服务网格？ 服务网格并没有给我们带来新功能，它是用于解决其他工具已经解决过的问题，只不过这次是在云原生的 Kubernetes 环境下的实现。 在传统的 MVC 三层 Web 应用程序架构下，服务之间的通讯并不复杂，在应用程序内部自己管理即可，但是在现今的复杂的大型网站情况下，单体应用被分解为众多的微服务，服务之间的依赖和通讯十分复杂，出现了 twitter 开发的 Finagle、Netflix 开发的 Hystrix 和 Google 的 Stubby 这样的 “胖客户端” 库，这些就是早期的服务网格，但是它们都仅适用于特定的环境和特定的开发语言，并不能作为平台级的服务网格支持。 在云原生架构下，容器的使用给予了异构应用程序的更多可行性，Kubernetes 增强的应用的横向扩容能力，用户可以快速的编排出复杂环境、复杂依赖关系的应用程序，同时开发者又无须过分关心应用程序的监控、扩展性、服务发现和分布式追踪这些繁琐的事情而专注于程序开发，赋予开发者更多的创造性。 参考 WHAT’S A SERVICE MESH? AND WHY DO I NEED ONE? - buoyant.io Istio: A service mesh for AWS ECS - medium.com 初次了解 Istio - istio.io Application Network Functions With ESBs, API Management, and Now.. Service Mesh? - blog.christianposta.com Pattern: Service Mesh - philcalcado.com Envoy 官方文档中文版 - servicemesher.com Istio 官方文档 - istio.io servicemesher/awesome-servicemesh - github.com 点击关注【ServiceMesher】 微信公众号回复【加群】加入社区Copyright © servicemesher.com 2018-2019 all right reserved，powered by Gitbook Updated at 2019-03-13 17:50:44 "},"concepts-and-principle/service-mesh-architectures.html":{"url":"concepts-and-principle/service-mesh-architectures.html","title":"服务网格架构","summary":"本文介绍了服务网格架构。","keywords":"","body":"服务网格架构 下图是Conduit Service Mesh（现在已合并到Linkerd2中了）的架构图，这是Service Mesh的一种典型的架构。 图片 - 服务网格架构示意图 服务网格中分为控制平面和数据平面，当前流行的两款开源的服务网格 Istio 和 Linkerd 实际上都是这种架构，只不过 Istio 的划分更清晰，而且部署更零散，很多组件都被拆分，控制平面中包括 Mixer、Pilot、Citadel，数据平面默认是用 Envoy；而 Linkerd 中只分为 Linkerd 做数据平面，namerd 作为控制平面。 控制平面 控制平面的特点： 不直接解析数据包 与数据平面中的代理通信，下发策略和配置 负责网络行为的可视化 通常提供 API 或者命令行工具可用于配置版本化管理，便于持续集成和部署 数据平面 数据平面的特点： 通常是按照无状态目标设计的，但实际上为了提高流量转发性能，需要缓存一些数据，因此无状态也是有争议的 直接处理入站和出站数据包，转发、路由、健康检查、负载均衡、认证、鉴权、产生监控数据等 对应用来说透明，即可以做到无感知部署 参考 企业级服务网格架构之路解读 点击关注【ServiceMesher】 微信公众号回复【加群】加入社区Copyright © servicemesher.com 2018-2019 all right reserved，powered by Gitbook Updated at 2019-03-15 10:34:41 "},"concepts-and-principle/service-mesh-patterns.html":{"url":"concepts-and-principle/service-mesh-patterns.html","title":"服务网格的实现模式","summary":"本文介绍了服务网格架构中的实现模式。","keywords":"","body":"服务网格的实现模式 我们在前面看到了通过客户端库来治理服务的架构图，那是我们在改造成 Service Mesh 架构前使用微服务架构通常的形式，下图是使用 Service Mesh 架构的最终形式。 图片 - Service Mesh 架构示意图 当然在达到这一最终形态之前我们需要将架构一步步演进，下面给出的是参考的演进路线。 Ingress 或边缘代理 如果你使用的是 Kubernetes 做容器编排调度，那么在进化到 Service Mesh 架构之前，通常会使用 Ingress Controller，做集群内外流量的反向代理，如使用 Traefik 或 Nginx Ingress Controller。 图片 - Ingress 或边缘代理架构示意图 这样只要利用 Kubernetes 的原有能力，当你的应用微服务化并容器化需要开放外部访问且只需要 L7 代理的话这种改造十分简单，但问题是无法管理服务间流量。 路由器网格 Ingress 或者边缘代理可以处理进出集群的流量，为了应对集群内的服务间流量管理，我们可以在集群内加一个 Router 层，即路由器层，让集群内所有服务间的流量都通过该路由器。 图片 - 路由器网格架构示意图 这个架构无需对原有的单体应用和新的微服务应用做什么改造，可以很轻易的迁移进来，但是当服务多了管理起来就很麻烦。 Proxy per Node 这种架构是在每个节点上都部署一个代理，如果使用 Kubernetes 来部署的话就是使用 DaemonSet 对象，Linkerd 第一代就是使用这种方式部署的，一代的 Linkerd 使用 Scala 开发，基于 JVM 比较消耗资源，二代的 Linkerd 使用 Go 开发。 图片 - Proxy per node 架构示意图 这种架构有个好处是每个节点只需要部署一个代理即可，比起在每个应用中都注入一个 sidecar 的方式更节省资源，而且更适合基于物理机/虚拟机的大型单体应用，但是也有一些副作用，比如粒度还是不够细，如果一个节点出问题，该节点上的所有服务就都会无法访问，对于服务来说不是完全透明的。 Sidecar 代理/Fabric 模型 这个一般不会成为典型部署类型，当企业的服务网格架构演进到这一步时通常只会持续很短时间，然后就会增加控制平面。跟前几个阶段最大的不同就是，应用程序和代理被放在了同一个部署单元里，可以对应用程序的流量做更细粒度的控制。 图片 - Sidecar 代理/Fabric模型示意图 这已经是最接近 Service Mesh 架构的一种形态了，唯一缺的就是控制平面了。所有的 sidecar 都支持热加载，配置的变更可以很容易的在流量控制中反映出来，但是如何操作这么多 sidecar 就需要一个统一的控制平面了。 Sidecar 代理/控制平面 下面的示意图是目前大多数 Service Mesh 的架构图，也可以说是整个 Service Mesh 架构演进的最终形态。 图片 - Sidecar 代理/控制平面架构示意图 这种架构将代理作为整个服务网格中的一部分，使用 Kubernetes 部署的话，可以通过以 sidecar 的形式注入，减轻了部署的负担，可以对每个服务做细粒度权限与流量控制。但有一点不好就是为每个服务都注入一个代理会占用很多资源，因此要想方设法降低每个代理的资源消耗。 多集群部署和扩展 以上都是单个服务网格集群的架构，所有的服务都位于同一个集群中，服务网格管理进出集群和集群内部的流量，当我们需要管理多个集群或者是引入外部的服务时就需要网格扩展和多集群配置。 参考 企业级服务网格架构之路解读 点击关注【ServiceMesher】 微信公众号回复【加群】加入社区Copyright © servicemesher.com 2018-2019 all right reserved，powered by Gitbook Updated at 2019-03-15 11:26:01 "},"concepts-and-principle/istio-architecture.html":{"url":"concepts-and-principle/istio-architecture.html","title":"Istio 架构解析","summary":"本文从高层次上介绍了 Istio 的架构。","keywords":"","body":"Istio 架构解析 下面是以漫画的形式说明 Istio 是什么。 图片 - 来自 Twitter @daniseyu21 图片 - 来自 Blog @sunny0826 Istio 是独立于平台的，可以在 Kubernetes 、 Consul 、虚拟机上部署的服务 Istio 的组成 Envoy：智能代理、流量控制 Pilot：服务发现、流量管理 Mixer：访问控制、遥测 Citadel：终端用户认证、流量加密 Galley（1.1新增）：验证、处理和分配配置 Service mesh 关注的方面 可观察性 安全性 可运维性 可拓展性 Istio 的策略执行组件可以扩展和定制，同时也是可拔插的 Istio 在数据平面为每个服务中注入一个 Envoy 代理以 Sidecar 形式运行来劫持所有进出服务的流量，同时对流量加以控制，通俗的讲就是应用程序你只管处理你的业务逻辑，其他的事情 Sidecar 会汇报给 Istio 控制平面处理 应用程序只需关注于业务逻辑（这才能生钱）即可，非功能性需求交给 Istio 设计目标 Istio 的架构设计中有几个关键目标，这些目标对于使系统能够应对大规模流量和高性能地服务处理至关重要。 最大化透明度：若想 Istio 被采纳，应该让运维和开发人员只需付出很少的代价就可以从中受益。为此，Istio 将自身自动注入到服务间所有的网络路径中。Istio 使用 sidecar 代理来捕获流量，并且在尽可能的地方自动编程网络层，以路由流量通过这些代理，而无需对已部署的应用程序代码进行任何改动。在 Kubernetes中，代理被注入到 pod 中，通过编写 iptables 规则来捕获流量。注入 sidecar 代理到 pod 中并且修改路由规则后，Istio 就能够调解所有流量。这个原则也适用于性能。当将 Istio 应用于部署时，运维人员可以发现，为提供这些功能而增加的资源开销是很小的。所有组件和 API 在设计时都必须考虑性能和规模。 可扩展性：随着运维人员和开发人员越来越依赖 Istio 提供的功能，系统必然和他们的需求一起成长。虽然我们期望继续自己添加新功能，但是我们预计最大的需求是扩展策略系统，集成其他策略和控制来源，并将网格行为信号传播到其他系统进行分析。策略运行时支持标准扩展机制以便插入到其他服务中。此外，它允许扩展词汇表，以允许基于网格生成的新信号来执行策略。 可移植性：使用 Istio 的生态系统将在很多维度上有差异。Istio 必须能够以最少的代价运行在任何云或预置环境中。将基于 Istio 的服务移植到新环境应该是轻而易举的，而使用 Istio 将一个服务同时部署到多个环境中也是可行的（例如，在多个云上进行冗余部署）。 策略一致性：在服务间的 API 调用中，策略的应用使得可以对网格间行为进行全面的控制，但对于无需在 API 级别表达的资源来说，对资源应用策略也同样重要。例如，将配额应用到 ML 训练任务消耗的 CPU 数量上，比将配额应用到启动这个工作的调用上更为有用。因此，策略系统作为独特的服务来维护，具有自己的 API，而不是将其放到代理/sidecar 中，这容许服务根据需要直接与其集成。 参考 Isito 是什么? - istio.io Bookinfo 示例 点击关注【ServiceMesher】 微信公众号回复【加群】加入社区Copyright © servicemesher.com 2018-2019 all right reserved，powered by Gitbook Updated at 2019-03-21 10:17:59 "},"concepts-and-principle/sidecar-pattern.html":{"url":"concepts-and-principle/sidecar-pattern.html","title":"Sidecar 模式","summary":"本文介绍了 service mesh 中的 sidecar 模式。","keywords":"","body":"Sidecar 模式 Sidecar 模式是 Istio 服务网格采用的模式，在服务网格出现之前该模式就一直存在，尤其是当微服务出现后开始盛行，本文讲解 Sidecar 模式。 什么是 Sidecar 模式 将应用程序的功能划分为单独的进程可以被视为 Sidecar 模式。Sidecar 设计模式允许你为应用程序添加许多功能，而无需额外第三方组件的配置和代码。 就如 Sidecar 连接着摩托车一样，类似地在软件架构中， Sidecar 应用是连接到父应用并且为其扩展或者增强功能。Sidecar 应用与主应用程序松散耦合。 让我用一个例子解释一下。想象一下假如你有6个微服务相互通信以确定一个包裹的成本。 每个微服务都需要具有可观察性、监控、日志记录、配置、断路器等功能。所有这些功能都是根据一些行业标准的第三方库在每个微服务中实现的。 但再想一想，这不是多余吗？它不会增加应用程序的整体复杂性吗？如果你的应用程序是用不同的语言编写时会发生什么——如何合并那些特定用于 .Net、Java、Python 等语言的第三方库。 使用 Sidecar 模式的优势 通过抽象出与功能相关的共同基础设施到一个不同层降低了微服务代码的复杂度。 因为你不再需要编写相同的第三方组件配置文件和代码，所以能够降低微服务架构中的代码重复度。 降低应用程序代码和底层平台的耦合度。 Sidecar 模式如何工作 Sidecar 是容器应用模式的一种，也是在 Service Mesh 中发扬光大的一种模式，详见 Service Mesh 架构解析，其中详细描述了节点代理和 Sidecar 模式的 Service Mesh 架构。 使用 Sidecar 模式部署服务网格时，无需在节点上运行代理（因此您不需要基础结构的协作），但是集群中将运行多个相同的 Sidecar 副本。从另一个角度看：我可以为一组微服务部署到一个服务网格中，你也可以部署一个有特定实现的服务网格。在 Sidecar 部署方式中，你会为每个应用的容器部署一个伴生容器。Sidecar 接管进出应用容器的所有流量。在 Kubernetes 的 Pod 中，在原有的应用容器旁边运行一个 Sidecar 容器，可以理解为两个容器共享存储、网络等资源，可以广义的将这个注入了 Sidecar 容器的 Pod 理解为一台主机，两个容器共享主机资源。 例如下图 SOFAMesh & SOFA MOSN—基于 Istio 构建的用于应对大规模流量的 Service Mesh 解决方案的架构图中描述的，MOSN 作为 Sidecar 的方式和应用运行在同一个 Pod 中，拦截所有进出应用容器的流量，SOFAMesh 兼容 Istio，其中使用 Go 语言开发的 SOFAMosn 替换了 Envoy。 图片 - SOFAMesh 架构图 参考 理解 Istio Service Mesh 中 Envoy 代理 Sidecar 注入及流量劫持 - jimmysong.io 微服务中的 Sidecar 设计模式解析 - servicemesher.com 点击关注【ServiceMesher】 微信公众号回复【加群】加入社区Copyright © servicemesher.com 2018-2019 all right reserved，powered by Gitbook Updated at 2019-03-13 11:06:27 "},"concepts-and-principle/sidecar-injection-deep-dive.html":{"url":"concepts-and-principle/sidecar-injection-deep-dive.html","title":"Istio 中的 Sidecar 注入与流量劫持详解","summary":"本文介绍了 istio service mesh 中的 sidecar 注入及流量劫持的详细过程。","keywords":"","body":"Istio 中的 Sidecar 注入与流量劫持详解 在讲解 Istio 如何将 Envoy 代理注入到应用程序 Pod 中之前，我们需要先了解以下几个概念： Sidecar 模式：容器应用模式之一，Service Mesh 架构的一种实现方式。 Init 容器：Pod 中的一种专用的容器，在应用程序容器启动之前运行，用来包含一些应用镜像中不存在的实用工具或安装脚本。 iptables：流量劫持是通过 iptables 转发实现的。 Init 容器 Init 容器是一种专用容器，它在应用程序容器启动之前运行，用来包含一些应用镜像中不存在的实用工具或安装脚本。 一个 Pod 中可以指定多个 Init 容器，如果指定了多个，那么 Init 容器将会按顺序依次运行。只有当前面的 Init 容器必须运行成功后，才可以运行下一个 Init 容器。当所有的 Init 容器运行完成后，Kubernetes 才初始化 Pod 和运行应用容器。 Init 容器使用 Linux Namespace，所以相对应用程序容器来说具有不同的文件系统视图。因此，它们能够具有访问 Secret 的权限，而应用程序容器则不能。 在 Pod 启动过程中，Init 容器会按顺序在网络和数据卷初始化之后启动。每个容器必须在下一个容器启动之前成功退出。如果由于运行时或失败退出，将导致容器启动失败，它会根据 Pod 的 restartPolicy 指定的策略进行重试。然而，如果 Pod 的 restartPolicy 设置为 Always，Init 容器失败时会使用 RestartPolicy 策略。 在所有的 Init 容器没有成功之前，Pod 将不会变成 Ready 状态。Init 容器的端口将不会在 Service 中进行聚集。 正在初始化中的 Pod 处于 Pending 状态，但应该会将 Initializing 状态设置为 true。Init 容器运行完成以后就会自动终止。 关于 Init 容器的详细信息请参考 Init 容器 - Kubernetes 中文指南/云原生应用架构实践手册。 Sidecar 注入示例分析 我们看下 Istio 官方示例 bookinfo 中 productpage 的 YAML 配置，关于 bookinfo 应用的详细 YAML 配置请参考 bookinfo.yaml。 apiVersion: v1 kind: Service metadata: name: productpage labels: app: productpage spec: ports: - port: 9080 name: http selector: app: productpage --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: productpage-v1 spec: replicas: 1 template: metadata: labels: app: productpage version: v1 spec: containers: - name: productpage image: istio/examples-bookinfo-productpage-v1:1.8.0 imagePullPolicy: IfNotPresent ports: - containerPort: 9080 再查看下 productpage 容器的 Dockerfile。 FROM python:2.7-slim COPY requirements.txt ./ RUN pip install --no-cache-dir -r requirements.txt COPY productpage.py /opt/microservices/ COPY templates /opt/microservices/templates COPY requirements.txt /opt/microservices/ EXPOSE 9080 WORKDIR /opt/microservices CMD python productpage.py 9080 我们看到 Dockerfile 中没有配置 ENTRYPOINT，所以 CMD 的配置 python productpage.py 9080 将作为默认的 ENTRYPOINT，记住这一点，再看下注入 sidecar 之后的配置。 $ istioctl kube-inject -f yaml/istio-bookinfo/bookinfo.yaml 我们只截取其中与 productpage 相关的 Service 和 Deployment 配置部分。 apiVersion: v1 kind: Service metadata: name: productpage labels: app: productpage spec: ports: - port: 9080 name: http selector: app: productpage --- apiVersion: extensions/v1beta1 kind: Deployment metadata: creationTimestamp: null name: productpage-v1 spec: replicas: 1 strategy: {} template: metadata: annotations: sidecar.istio.io/status: '{\"version\":\"fde14299e2ae804b95be08e0f2d171d466f47983391c00519bbf01392d9ad6bb\",\"initContainers\":[\"istio-init\"],\"containers\":[\"istio-proxy\"],\"volumes\":[\"istio-envoy\",\"istio-certs\"],\"imagePullSecrets\":null}' creationTimestamp: null labels: app: productpage version: v1 spec: containers: - image: istio/examples-bookinfo-productpage-v1:1.8.0 imagePullPolicy: IfNotPresent name: productpage ports: - containerPort: 9080 resources: {} - args: - proxy - sidecar - --configPath - /etc/istio/proxy - --binaryPath - /usr/local/bin/envoy - --serviceCluster - productpage - --drainDuration - 45s - --parentShutdownDuration - 1m0s - --discoveryAddress - istio-pilot.istio-system:15007 - --discoveryRefreshDelay - 1s - --zipkinAddress - zipkin.istio-system:9411 - --connectTimeout - 10s - --statsdUdpAddress - istio-statsd-prom-bridge.istio-system:9125 - --proxyAdminPort - \"15000\" - --controlPlaneAuthPolicy - NONE env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: INSTANCE_IP valueFrom: fieldRef: fieldPath: status.podIP - name: ISTIO_META_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: ISTIO_META_INTERCEPTION_MODE value: REDIRECT image: jimmysong/istio-release-proxyv2:1.0.0 imagePullPolicy: IfNotPresent name: istio-proxy resources: requests: cpu: 10m securityContext: privileged: false readOnlyRootFilesystem: true runAsUser: 1337 volumeMounts: - mountPath: /etc/istio/proxy name: istio-envoy - mountPath: /etc/certs/ name: istio-certs readOnly: true initContainers: - args: - -p - \"15001\" - -u - \"1337\" - -m - REDIRECT - -i - '*' - -x - \"\" - -b - 9080, - -d - \"\" image: jimmysong/istio-release-proxy_init:1.0.0 imagePullPolicy: IfNotPresent name: istio-init resources: {} securityContext: capabilities: add: - NET_ADMIN privileged: true volumes: - emptyDir: medium: Memory name: istio-envoy - name: istio-certs secret: optional: true secretName: istio.default status: {} 我们看到 Service 的配置没有变化，所有的变化都在 Deployment 里，Istio 给应用 Pod 注入的配置主要包括： Init 容器 istio-init：用于给 Sidecar 容器即 Envoy 代理做初始化，设置 iptables 端口转发 Envoy sidecar 容器 istio-proxy：运行 Envoy 代理 接下来将分别解析下这两个容器。 Init 容器解析 Istio 在 Pod 中注入的 Init 容器名为 istio-init，我们在上面 Istio 注入完成后的 YAML 文件中看到了该容器的启动参数： -p 15001 -u 1337 -m REDIRECT -i '*' -x \"\" -b 9080 -d \"\" 我们再检查下该容器的 Dockerfile 看看 ENTRYPOINT 是什么以确定启动时执行的命令。 FROM ubuntu:xenial RUN apt-get update && apt-get install -y \\ iproute2 \\ iptables \\ && rm -rf /var/lib/apt/lists/* ADD istio-iptables.sh /usr/local/bin/ ENTRYPOINT [\"/usr/local/bin/istio-iptables.sh\"] 我们看到 istio-init 容器的入口是 /usr/local/bin/istio-iptables.sh 脚本，再按图索骥看看这个脚本里到底写的什么，该脚本的位置在 Istio 源码仓库的 tools/deb/istio-iptables.sh，一共 300 多行，就不贴在这里了。下面我们就来解析下这个启动脚本。 Init 容器启动入口 Init 容器的启动入口是 /usr/local/bin/istio-iptables.sh 脚本，该脚本的用法如下： $ istio-iptables.sh -p PORT -u UID -g GID [-m mode] [-b ports] [-d ports] [-i CIDR] [-x CIDR] [-h] -p: 指定重定向所有 TCP 流量的 Envoy 端口（默认为 $ENVOY_PORT = 15001） -u: 指定未应用重定向的用户的 UID。通常，这是代理容器的 UID（默认为 $ENVOY_USER 的 uid，istio_proxy 的 uid 或 1337） -g: 指定未应用重定向的用户的 GID。（与 -u param 相同的默认值） -m: 指定入站连接重定向到 Envoy 的模式，“REDIRECT” 或 “TPROXY”（默认为 $ISTIO_INBOUND_INTERCEPTION_MODE) -b: 逗号分隔的入站端口列表，其流量将重定向到 Envoy（可选）。使用通配符 “*” 表示重定向所有端口。为空时表示禁用所有入站重定向（默认为 $ISTIO_INBOUND_PORTS） -d: 指定要从重定向到 Envoy 中排除（可选）的入站端口列表，以逗号格式分隔。使用通配符“*” 表示重定向所有入站流量（默认为 $ISTIO_LOCAL_EXCLUDE_PORTS） -i: 指定重定向到 Envoy（可选）的 IP 地址范围，以逗号分隔的 CIDR 格式列表。使用通配符 “*” 表示重定向所有出站流量。空列表将禁用所有出站重定向（默认为 $ISTIO_SERVICE_CIDR） -x: 指定将从重定向中排除的 IP 地址范围，以逗号分隔的 CIDR 格式列表。使用通配符 “*” 表示重定向所有出站流量（默认为 $ISTIO_SERVICE_EXCLUDE_CIDR）。 环境变量位于 $ISTIO_SIDECAR_CONFIG（默认在：/var/lib/istio/envoy/sidecar.env） 通过查看该脚本你将看到，以上传入的参数都会重新组装成 iptables 命令的参数。 再参考 istio-init 容器的启动参数，完整的启动命令如下： $ /usr/local/bin/istio-iptables.sh -p 15001 -u 1337 -m REDIRECT -i '*' -x \"\" -b 9080 -d \"\" 该容器存在的意义就是让 Envoy 代理可以拦截所有的进出 Pod 的流量，即将入站流量重定向到 Sidecar，再拦截应用容器的出站流量经过 Sidecar 处理后再出站。 命令解析 这条启动命令的作用是： 将应用容器的所有流量都转发到 Envoy 的 15001 端口。 使用 istio-proxy 用户身份运行， UID 为 1337，即 Envoy 所处的用户空间，这也是 istio-proxy 容器默认使用的用户，见 YAML 配置中的 runAsUser 字段。 使用默认的 REDIRECT 模式来重定向流量。 将所有出站流量都重定向到 Envoy 代理。 将所有访问 9080 端口（即应用容器 productpage 的端口）的流量重定向到 Envoy 代理。 因为 Init 容器初始化完毕后就会自动终止，因为我们无法登陆到容器中查看 iptables 信息，但是 Init 容器初始化结果会保留到应用容器和 Sidecar 容器中。 istio-proxy 容器解析 为了查看 iptables 配置，我们需要登陆到 Sidecar 容器中使用 root 用户来查看，因为 kubectl 无法使用特权模式来远程操作 docker 容器，所以我们需要登陆到 productpage Pod 所在的主机上使用 docker 命令登陆容器中查看。 查看 productpage Pod 所在的主机。 $ kubectl -n default get pod -l app=productpage -o wide NAME READY STATUS RESTARTS AGE IP NODE productpage-v1-745ffc55b7-2l2lw 2/2 Running 0 1d 172.33.78.10 node3 从输出结果中可以看到该 Pod 运行在 node3 上，使用 vagrant 命令登陆到 node3 主机中并切换为 root 用户。 $ vagrant ssh node3 $ sudo -i 查看 iptables 配置，列出 NAT（网络地址转换）表的所有规则，因为在 Init 容器启动的时候选择给 istio-iptables.sh 传递的参数中指定将入站流量重定向到 Envoy 的模式为 “REDIRECT”，因此在 iptables 中将只有 NAT 表的规格配置，如果选择 TPROXY 还会有 mangle 表配置。iptables 命令的详细用法请参考 iptables，规则配置请参考 iptables 规则配置。 理解 iptables iptables 是 Linux 内核中的防火墙软件 netfilter 的管理工具，位于用户空间，同时也是 netfilter 的一部分。Netfilter 位于内核空间，不仅有网络地址转换的功能，也具备数据包内容修改、以及数据包过滤等防火墙功能。 在了解 Init 容器初始化的 iptables 之前，我们先来了解下 iptables 和规则配置。 下图展示了 iptables 调用链。 图片 - iptables 调用链 iptables 中的表 Init 容器中使用的的 iptables 版本是 v1.6.0，共包含 5 张表： raw 用于配置数据包，raw 中的数据包不会被系统跟踪。 filter 是用于存放所有与防火墙相关操作的默认表。 nat 用于 网络地址转换（例如：端口转发）。 mangle 用于对特定数据包的修改（参考损坏数据包）。 security 用于强制访问控制 网络规则。 注：在本示例中只用到了 nat 表。 不同的表中的具有的链类型如下表所示： 规则名称 raw filter nat mangle security PREROUTING ✓ ✓ ✓ INPUT ✓ ✓ ✓ ✓ OUTPUT ✓ ✓ ✓ ✓ POSTROUTING ✓ ✓ FORWARD ✓ ✓ ✓ ✓ 下图是 iptables 的调用链顺序。 图片 - iptables 调用链 关于 iptables 的详细介绍请参考常见 iptables 使用规则场景整理。 iptables 命令 iptables 命令的主要用途是修改这些表中的规则。iptables 命令格式如下： $ iptables [-t 表名] 命令选项［链名]［条件匹配］[-j 目标动作或跳转］ Init 容器中的 /istio-iptables.sh 启动入口脚本就是执行 iptables 初始化的。 理解 iptables 规则 查看 istio-proxy 容器中的默认的 iptables 规则，默认查看的是 filter 表中的规则。 $ iptables -L -v Chain INPUT (policy ACCEPT 350K packets, 63M bytes) pkts bytes target prot opt in out source destination Chain FORWARD (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination Chain OUTPUT (policy ACCEPT 18M packets, 1916M bytes) pkts bytes target prot opt in out source destination 我们看到三个默认的链，分别是 INPUT、FORWARD 和 OUTPUT，每个链中的第一行输出表示链名称（在本例中为INPUT/FORWARD/OUTPUT），后跟默认策略（ACCEPT）。 下图是 iptables 的建议结构图，流量在经过 INPUT 链之后就进入了上层协议栈，比如 图片 - iptables结构图 图片来自常见 iptables 使用规则场景整理 每条链中都可以添加多条规则，规则是按照顺序从前到后执行的。我们来看下规则的表头定义。 pkts：处理过的匹配的报文数量 bytes：累计处理的报文大小（字节数） target：如果报文与规则匹配，指定目标就会被执行。 prot：协议，例如 tdp、udp、icmp 和 all。 opt：很少使用，这一列用于显示 IP 选项。 in：入站网卡。 out：出站网卡。 source：流量的源 IP 地址或子网，后者是 anywhere。 destination：流量的目的地 IP 地址或子网，或者是 anywhere。 还有一列没有表头，显示在最后，表示规则的选项，作为规则的扩展匹配条件，用来补充前面的几列中的配置。prot、opt、in、out、source 和 destination 和显示在 destination 后面的没有表头的一列扩展条件共同组成匹配规则。当流量匹配这些规则后就会执行 target。 关于 iptables 规则请参考常见iptables使用规则场景整理。 target 支持的类型 target 类型包括 ACCEPT、REJECT、DROP、LOG 、SNAT、MASQUERADE、DNAT、REDIRECT、RETURN 或者跳转到其他规则等。只要执行到某一条链中只有按照顺序有一条规则匹配后就可以确定报文的去向了，除了 RETURN 类型，类似编程语言中的 return 语句，返回到它的调用点，继续执行下一条规则。target 支持的配置详解请参考 iptables 详解（1）：iptables 概念。 从输出结果中可以看到 Init 容器没有在 iptables 的默认链路中创建任何规则，而是创建了新的链路。 查看 iptables nat 表中注入的规则 Init 容器通过向 iptables nat 表中注入转发规则来劫持流量的，下图显示的是 productpage 服务中的 iptables 流量劫持的详细过程。 图片 - Envoy sidecar 流量劫持流程示意图 Init 容器启动时命令行参数中指定了 REDIRECT 模式，因此只创建了 NAT 表规则，接下来我们查看下 NAT 表中创建的规则，这是全文中的重点部分，前面讲了那么多都是为它做铺垫的。下面是查看 nat 表中的规则，其中链的名字中包含 ISTIO 前缀的是由 Init 容器注入的，规则匹配是根据下面显示的顺序来执行的，其中会有多次跳转。 # 查看 NAT 表中规则配置的详细信息 $ iptables -t nat -L -v # PREROUTING 链：用于目标地址转换（DNAT），将所有入站 TCP 流量跳转到 ISTIO_INBOUND 链上 Chain PREROUTING (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination 2 120 ISTIO_INBOUND tcp -- any any anywhere anywhere # INPUT 链：处理输入数据包，非 TCP 流量将继续 OUTPUT 链 Chain INPUT (policy ACCEPT 2 packets, 120 bytes) pkts bytes target prot opt in out source destination # OUTPUT 链：将所有出站数据包跳转到 ISTIO_OUTPUT 链上 Chain OUTPUT (policy ACCEPT 41146 packets, 3845K bytes) pkts bytes target prot opt in out source destination 93 5580 ISTIO_OUTPUT tcp -- any any anywhere anywhere # POSTROUTING 链：所有数据包流出网卡时都要先进入POSTROUTING 链，内核根据数据包目的地判断是否需要转发出去，我们看到此处未做任何处理 Chain POSTROUTING (policy ACCEPT 41199 packets, 3848K bytes) pkts bytes target prot opt in out source destination # ISTIO_INBOUND 链：将所有目的地为 9080 端口的入站流量重定向到 ISTIO_IN_REDIRECT 链上 Chain ISTIO_INBOUND (1 references) pkts bytes target prot opt in out source destination 2 120 ISTIO_IN_REDIRECT tcp -- any any anywhere anywhere tcp dpt:9080 # ISTIO_IN_REDIRECT 链：将所有的入站流量跳转到本地的 15001 端口，至此成功的拦截了流量到 Envoy Chain ISTIO_IN_REDIRECT (1 references) pkts bytes target prot opt in out source destination 2 120 REDIRECT tcp -- any any anywhere anywhere redir ports 15001 # ISTIO_OUTPUT 链：选择需要重定向到 Envoy（即本地） 的出站流量，所有非 localhost 的流量全部转发到 ISTIO_REDIRECT。为了避免流量在该 Pod 中无限循环，所有到 istio-proxy 用户空间的流量都返回到它的调用点中的下一条规则，本例中即 OUTPUT 链，因为跳出 ISTIO_OUTPUT 规则之后就进入下一条链 POSTROUTING。如果目的地非 localhost 就跳转到 ISTIO_REDIRECT；如果流量是来自 istio-proxy 用户空间的，那么就跳出该链，返回它的调用链继续执行下一条规则（OUPT 的下一条规则，无需对流量进行处理）；所有的非 istio-proxy 用户空间的目的地是 localhost 的流量就跳转到 ISTIO_REDIRECT Chain ISTIO_OUTPUT (1 references) pkts bytes target prot opt in out source destination 0 0 ISTIO_REDIRECT all -- any lo anywhere !localhost 40 2400 RETURN all -- any any anywhere anywhere owner UID match istio-proxy 0 0 RETURN all -- any any anywhere anywhere owner GID match istio-proxy 0 0 RETURN all -- any any anywhere localhost 53 3180 ISTIO_REDIRECT all -- any any anywhere anywhere # ISTIO_REDIRECT 链：将所有流量重定向到 Envoy（即本地） 的 15001 端口 Chain ISTIO_REDIRECT (2 references) pkts bytes target prot opt in out source destination 53 3180 REDIRECT tcp -- any any anywhere anywhere redir ports 15001 iptables 显示的链的顺序，即流量规则匹配的顺序。其中要特别注意 ISTIO_OUTPUT 链中的规则配置。为了避免流量一直在 Pod 中无限循环，所有到 istio-proxy 用户空间的流量都返回到它的调用点中的下一条规则，本例中即 OUTPUT 链，因为跳出 ISTIO_OUTPUT 规则之后就进入下一条链 POSTROUTING。 ISTIO_OUTPUT 链规则匹配的详细过程如下： 如果目的地非 localhost 就跳转到 ISTIO_REDIRECT 链 所有来自 istio-proxy 用户空间的非 localhost 流量跳转到它的调用点 OUTPUT 继续执行 OUTPUT 链的下一条规则，因为 OUTPUT 链中没有下一条规则了，所以会继续执行 POSTROUTING 链然后跳出 iptables，直接访问目的地 如果流量不是来自 istio-proxy 用户空间，又是对 localhost 的访问，那么就跳出 iptables，直接访问目的地 其它所有情况都跳转到 ISTIO_REDIRECT 链 其实在最后这条规则前还可以增加 IP 地址过滤，让某些 IP 地址段不通过 Envoy 代理。 图片 - istio sidecar iptables 注入 以上 iptables 规则都是 Init 容器启动的时使用 istio-iptables.sh 脚本生成的，详细过程可以查看该脚本。 查看 Envoy 运行状态 首先查看 proxyv2 镜像的 Dockerfile。 FROM istionightly/base_debug ARG proxy_version ARG istio_version # 安装 Envoy ADD envoy /usr/local/bin/envoy # 使用环境变量的方式明文指定 proxy 的版本/功能 ENV ISTIO_META_ISTIO_PROXY_VERSION \"1.1.0\" # 使用环境变量的方式明文指定 proxy 明确的 sha，用于指定版本的配置和调试 ENV ISTIO_META_ISTIO_PROXY_SHA $proxy_version # 环境变量，指定明确的构建号，用于调试 ENV ISTIO_META_ISTIO_VERSION $istio_version ADD pilot-agent /usr/local/bin/pilot-agent ADD envoy_pilot.yaml.tmpl /etc/istio/proxy/envoy_pilot.yaml.tmpl ADD envoy_policy.yaml.tmpl /etc/istio/proxy/envoy_policy.yaml.tmpl ADD envoy_telemetry.yaml.tmpl /etc/istio/proxy/envoy_telemetry.yaml.tmpl ADD istio-iptables.sh /usr/local/bin/istio-iptables.sh COPY envoy_bootstrap_v2.json /var/lib/istio/envoy/envoy_bootstrap_tmpl.json RUN chmod 755 /usr/local/bin/envoy /usr/local/bin/pilot-agent # 将 istio-proxy 用户加入 sudo 权限以允许执行 tcpdump 和其他调试命令 RUN useradd -m --uid 1337 istio-proxy && \\ echo \"istio-proxy ALL=NOPASSWD: ALL\" >> /etc/sudoers && \\ chown -R istio-proxy /var/lib/istio # 使用 pilot-agent 来启动 Envoy ENTRYPOINT [\"/usr/local/bin/pilot-agent\"] 该容器的启动入口是 pilot-agent 命令，根据 YAML 配置中传递的参数，详细的启动命令入下： /usr/local/bin/pilot-agent proxy sidecar --configPath /etc/istio/proxy --binaryPath /usr/local/bin/envoy --serviceCluster productpage --drainDuration 45s --parentShutdownDuration 1m0s --discoveryAddress istio-pilot.istio-system:15007 --discoveryRefreshDelay 1s --zipkinAddress zipkin.istio-system:9411 --connectTimeout 10s --statsdUdpAddress istio-statsd-prom-bridge.istio-system:9125 --proxyAdminPort 15000 --controlPlaneAuthPolicy NONE 主要配置了 Envoy 二进制文件的位置、服务发现地址、服务集群名、监控指标上报地址、Envoy 的管理端口、热重启时间等，详细用法请参考 Istio官方文档 pilot-agent 的用法。 pilot-agent 是容器中 PID 为 1 的启动进程，它启动时又创建了一个 Envoy 进程，如下： /usr/local/bin/envoy -c /etc/istio/proxy/envoy-rev0.json --restart-epoch 0 --drain-time-s 45 --parent-shutdown-time-s 60 --service-cluster productpage --service-node sidecar~172.33.78.10~productpage-v1-745ffc55b7-2l2lw.default~default.svc.cluster.local --max-obj-name-len 189 -l warn --v2-config-only 我们分别解释下以上配置的意义。 -c /etc/istio/proxy/envoy-rev0.json：配置文件，支持 .json、.yaml、.pb 和 .pb_text 格式，pilot-agent 启动的时候读取了容器的环境变量后创建的。 --restart-epoch 0：Envoy 热重启周期，第一次启动默认为 0，每热重启一次该值加 1。 --drain-time-s 45：热重启期间 Envoy 将耗尽连接的时间。 --parent-shutdown-time-s 60： Envoy 在热重启时关闭父进程之前等待的时间。 --service-cluster productpage：Envoy 运行的本地服务集群的名字。 --service-node sidecar~172.33.78.10~productpage-v1-745ffc55b7-2l2lw.default~default.svc.cluster.local：定义 Envoy 运行的本地服务节点名称，其中包含了该 Pod 的名称、IP、DNS 域等信息，根据容器的环境变量拼出来的。 -max-obj-name-len 189：cluster/route_config/listener 中名称字段的最大长度（以字节为单位） -l warn：日志级别 --v2-config-only：只解析 v2 引导配置文件 详细配置请参考 Envoy 的命令行选项。 查看 Envoy 的配置文件 /etc/istio/proxy/envoy-rev0.json。 { \"node\": { \"id\": \"sidecar~172.33.78.10~productpage-v1-745ffc55b7-2l2lw.default~default.svc.cluster.local\", \"cluster\": \"productpage\", \"metadata\": { \"INTERCEPTION_MODE\": \"REDIRECT\", \"ISTIO_PROXY_SHA\": \"istio-proxy:6166ae7ebac7f630206b2fe4e6767516bf198313\", \"ISTIO_PROXY_VERSION\": \"1.0.0\", \"ISTIO_VERSION\": \"1.0.0\", \"POD_NAME\": \"productpage-v1-745ffc55b7-2l2lw\", \"istio\": \"sidecar\" } }, \"stats_config\": { \"use_all_default_tags\": false }, \"admin\": { \"access_log_path\": \"/dev/stdout\", \"address\": { \"socket_address\": { \"address\": \"127.0.0.1\", \"port_value\": 15000 } } }, \"dynamic_resources\": { \"lds_config\": { \"ads\": {} }, \"cds_config\": { \"ads\": {} }, \"ads_config\": { \"api_type\": \"GRPC\", \"refresh_delay\": {\"seconds\": 1, \"nanos\": 0}, \"grpc_services\": [ { \"envoy_grpc\": { \"cluster_name\": \"xds-grpc\" } } ] } }, \"static_resources\": { \"clusters\": [ { \"name\": \"xds-grpc\", \"type\": \"STRICT_DNS\", \"connect_timeout\": {\"seconds\": 10, \"nanos\": 0}, \"lb_policy\": \"ROUND_ROBIN\", \"hosts\": [ { \"socket_address\": {\"address\": \"istio-pilot.istio-system\", \"port_value\": 15010} } ], \"circuit_breakers\": { \"thresholds\": [ { \"priority\": \"default\", \"max_connections\": \"100000\", \"max_pending_requests\": \"100000\", \"max_requests\": \"100000\" }, { \"priority\": \"high\", \"max_connections\": \"100000\", \"max_pending_requests\": \"100000\", \"max_requests\": \"100000\" }] }, \"upstream_connection_options\": { \"tcp_keepalive\": { \"keepalive_time\": 300 } }, \"http2_protocol_options\": { } } , { \"name\": \"zipkin\", \"type\": \"STRICT_DNS\", \"connect_timeout\": { \"seconds\": 1 }, \"lb_policy\": \"ROUND_ROBIN\", \"hosts\": [ { \"socket_address\": {\"address\": \"zipkin.istio-system\", \"port_value\": 9411} } ] } ] }, \"tracing\": { \"http\": { \"name\": \"envoy.zipkin\", \"config\": { \"collector_cluster\": \"zipkin\" } } }, \"stats_sinks\": [ { \"name\": \"envoy.statsd\", \"config\": { \"address\": { \"socket_address\": {\"address\": \"10.254.109.175\", \"port_value\": 9125} } } } ] } 下图是使用 Istio 管理的 bookinfo 示例的访问请求路径图。 图片 - Istio bookinfo 图片来自 Istio 官方网站 对照 bookinfo 示例的 productpage 查看建立的连接。在 productpage-v1-745ffc55b7-2l2lw Pod 的 istio-proxy 容器中使用 root 用户查看打开的端口。 $ lsof -i COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME envoy 11 istio-proxy 9u IPv4 73951 0t0 TCP localhost:15000 (LISTEN) # Envoy admin 端口 envoy 11 istio-proxy 17u IPv4 74320 0t0 TCP productpage-v1-745ffc55b7-2l2lw:46862->istio-pilot.istio-system.svc.cluster.local:15010 (ESTABLISHED) # 15010：istio-pilot 的 grcp-xds 端口 envoy 11 istio-proxy 18u IPv4 73986 0t0 UDP productpage-v1-745ffc55b7-2l2lw:44332->istio-statsd-prom-bridge.istio-system.svc.cluster.local:9125 # 给 Promethues 发送 metric 的端口 envoy 11 istio-proxy 52u IPv4 74599 0t0 TCP *:15001 (LISTEN) # Envoy 的监听端口 envoy 11 istio-proxy 53u IPv4 74600 0t0 UDP productpage-v1-745ffc55b7-2l2lw:48011->istio-statsd-prom-bridge.istio-system.svc.cluster.local:9125 # 给 Promethues 发送 metric 端口 envoy 11 istio-proxy 54u IPv4 338551 0t0 TCP productpage-v1-745ffc55b7-2l2lw:15001->172.17.8.102:52670 (ESTABLISHED) # 52670：Ingress gateway 端口 envoy 11 istio-proxy 55u IPv4 338364 0t0 TCP productpage-v1-745ffc55b7-2l2lw:44046->172.33.78.9:9091 (ESTABLISHED) # 9091：istio-telemetry 服务的 grpc-mixer 端口 envoy 11 istio-proxy 56u IPv4 338473 0t0 TCP productpage-v1-745ffc55b7-2l2lw:47210->zipkin.istio-system.svc.cluster.local:9411 (ESTABLISHED) # 9411: zipkin 端口 envoy 11 istio-proxy 58u IPv4 338383 0t0 TCP productpage-v1-745ffc55b7-2l2lw:41564->172.33.84.8:9080 (ESTABLISHED) # 9080：details-v1 的 http 端口 envoy 11 istio-proxy 59u IPv4 338390 0t0 TCP productpage-v1-745ffc55b7-2l2lw:54410->172.33.78.5:9080 (ESTABLISHED) # 9080：reivews-v2 的 http 端口 envoy 11 istio-proxy 60u IPv4 338411 0t0 TCP productpage-v1-745ffc55b7-2l2lw:35200->172.33.84.5:9091 (ESTABLISHED) # 9091:istio-telemetry 服务的 grpc-mixer 端口 envoy 11 istio-proxy 62u IPv4 338497 0t0 TCP productpage-v1-745ffc55b7-2l2lw:34402->172.33.84.9:9080 (ESTABLISHED) # reviews-v1 的 http 端口 envoy 11 istio-proxy 63u IPv4 338525 0t0 TCP productpage-v1-745ffc55b7-2l2lw:50592->172.33.71.5:9080 (ESTABLISHED) # reviews-v3 的 http 端口 从输出经过上可以验证 Sidecar 是如何接管流量和与 istio-pilot 通信，及向 Mixer 做遥测数据汇聚的。感兴趣的读者可以再去看看其他几个服务的 istio-proxy 容器中的 iptables 和端口信息。 参考 SOFAMesh & SOFA MOSN—基于Istio构建的用于应对大规模流量的Service Mesh解决方案 - jimmysong.io Init 容器 - Kubernetes 中文指南/云原生应用架构实践手册 - jimmysong.io JSONPath Support - kubernetes.io iptables 命令使用说明 - wangchujiang.com How To List and Delete Iptables Firewall Rules - digitalocean.com 一句一句解说 iptables的详细中文手册 - cnblog.com 常见iptables使用规则场景整理 - aliang.org 理解 Istio Service Mesh 中 Envoy 代理 Sidecar 注入及流量劫持 - jimmysong.io 点击关注【ServiceMesher】 微信公众号回复【加群】加入社区Copyright © servicemesher.com 2018-2019 all right reserved，powered by Gitbook Updated at 2019-03-13 11:06:27 "},"concepts-and-principle/istio-sidecar-injector.html":{"url":"concepts-and-principle/istio-sidecar-injector.html","title":"Sidecar 的自动注入过程详解","summary":"本文介绍了 istio service mesh 中的 sidecar 自动注入过程。","keywords":"","body":"Sidecar 的自动注入过程详解 在 Sidecar 注入与流量劫持详解中我只是简单介绍了 Sidecar 注入的步骤，但是没有涉及到具体的 Sidecar 注入流程与细节，这一篇将带大家了解 Istio 为数据平面自动注入 Sidecar 的详细过程。 Sidecar 注入过程 如 Istio 官方文档中对 Istio sidecar 注入的描述，你可以使用 istioctl 命令手动注入 Sidecar，也可以为 Kubernetes 集群自动开启 sidecar 注入，这主要用到了 Kubernetes 的准入控制器中的 webhook，参考 Istio 官网中对 Istio sidecar 注入的描述。 图片 - Sidecar 注入流程图 手动注入 sidecar 与自动注入 sidecar 的区别 不论是手动注入还是自动注入，sidecar 的注入过程都需要遵循如下步骤： Kubernetes 需要了解待注入的 sidecar 所连接的 Istio 集群及其配置； Kubernetes 需要了解待注入的 sidecar 容器本身的配置，如镜像地址、启动参数等； Kubernetes 根据 sidecar 注入模板和以上配置填充 sidecar 的配置参数，将以上配置注入到应用容器的一侧； Istio 和 sidecar 配置保存在 istio 和 istio-sidecar-injector 这两个 ConfigMap 中，其中包含了 Go template，所谓自动 sidecar 注入就是将生成 Pod 配置从应用 YAML 文件期间转移到 mutable webhook 中。 参考 注入 Istio sidecar - istio.io 点击关注【ServiceMesher】 微信公众号回复【加群】加入社区Copyright © servicemesher.com 2018-2019 all right reserved，powered by Gitbook Updated at 2019-03-13 11:06:27 "},"data-plane/":{"url":"data-plane/","title":"数据平面介绍","summary":"本文是数据平面的开篇。","keywords":"","body":"数据平面介绍 数据平面由一组以 sidecar 方式部署的智能代理组成。 这些代理可以调节和控制微服务之间所有的网络通信。 数据平面真正触及到对网络数据包的相关操作，是上层控制平面策略的具体执行者。 在服务网格中，数据平面 sidecar 代理主要负责执行如下任务： 服务发现：探测所有可用的上游或后端服务实例 健康检测：探测上游或后端服务实例是否健康，是否准备好接收网络流量 流量路由：将网络请求路由到正确的上游或后端服务 负载均衡：在对上游或后端服务进行请求时，选择合适的服务实例接收请求，同时负责处理超时、断路、重试等情况 身份验证和授权：对网络请求进行身份验证、权限验证，以决定是否响应以及如何响应，使用 mTLS 或其他机制对链路进行加密等 链路追踪：对于每个请求，生成详细的统计信息、日志记录和分布式追踪数据，以便操作人员能够理解调用路径并在出现问题时进行调试 简单来说，数据平面就是负责有条件地转换、转发以及观察进出服务实例的每个网络包。 典型的数据平面实现有：Linkerd、NGINX、HAProxy、Envoy、Traefik 参考 What is Istio? Service mesh data plane vs. control plane 点击关注【ServiceMesher】 微信公众号回复【加群】加入社区Copyright © servicemesher.com 2018-2019 all right reserved，powered by Gitbook Updated at 2019-03-13 13:41:37 "},"data-plane/envoy-terminology.html":{"url":"data-plane/envoy-terminology.html","title":"Envoy 中的基本术语","summary":"本文介绍了 Envoy 中的基本术语。","keywords":"","body":"Envoy 中的基本术语 Host：能够进行网络通信的实体（在手机或服务器等上的应用程序）。在 Envoy 中主机是指逻辑网络应用程序。只要每台主机都可以独立寻址，一块物理硬件上就运行多个主机。 Downstream：下游（downstream）主机连接到 Envoy，发送请求并获得响应。 Upstream：上游（upstream）主机获取来自 Envoy 的连接接请求和响应。 Cluster: 集群（cluster）是 Envoy 连接到的一组逻辑上相似的上游主机。Envoy 通过服务发现发现集群中的成员。Envoy 可以通过主动运行状况检查来确定集群成员的健康状况。Envoy 如何将请求路由到集群成员由负载均衡策略确定。 Mesh：一组互相协调以提供一致网络拓扑的主机。Envoy mesh 是指一组 Envoy 代理，它们构成了由多种不同服务和应用程序平台组成的分布式系统的消息传递基础。 运行时配置：与 Envoy 一起部署的带外实时配置系统。可以在无需重启 Envoy 或更改 Envoy 主配置的情况下，通过更改设置来影响操作。 Listener: 监听器（listener）是可以由下游客户端连接的命名网络位置（例如，端口、unix域套接字等）。Envoy 公开一个或多个下游主机连接的监听器。一般是每台主机运行一个 Envoy，使用单进程运行，但是每个进程中可以启动任意数量的 Listener（监听器），目前只监听 TCP，每个监听器都独立配置一定数量的（L3/L4）网络过滤器。Listenter 也可以通过 Listener Discovery Service（LDS）动态获取。 Listener filter：Listener 使用 listener filter（监听器过滤器）来操作连接的元数据。它的作用是在不更改 Envoy 的核心功能的情况下添加更多的集成功能。Listener filter 的 API 相对简单，因为这些过滤器最终是在新接受的套接字上运行。在链中可以互相衔接以支持更复杂的场景，例如调用速率限制。Envoy 已经包含了多个监听器过滤器。 Http Route Table：HTTP 的路由规则，例如请求的域名，Path 符合什么规则，转发给哪个 Cluster。 Health checking：健康检查会与SDS服务发现配合使用。但是，即使使用其他服务发现方式，也有相应需要进行主动健康检查的情况。详见 health checking。 参考 Envoy 的架构与基本术语 - jimmysong.io 点击关注【ServiceMesher】 微信公众号回复【加群】加入社区Copyright © servicemesher.com 2018-2019 all right reserved，powered by Gitbook Updated at 2019-03-13 17:48:25 "},"data-plane/istio-sidecar-proxy-config.html":{"url":"data-plane/istio-sidecar-proxy-config.html","title":"Istio sidecar proxy 配置","summary":"本文介绍了 Envoy 的基本配置。","keywords":"","body":"Istio sidecar proxy 配置 假如您使用 kubernetes-vagrant-centos-cluster 部署了 Kubernetes 集群并开启了 Istio Service Mesh，再部署 bookinfo 示例，那么在 default 命名空间下有一个名字类似于 ratings-v1-7c9949d479-dwkr4 的 Pod，使用下面的命令查看该 Pod 的 Envoy sidecar 的全量配置： kubectl -n default exec ratings-v1-7c9949d479-dwkr4 -c istio-proxy curl http://localhost:15000/config_dump > dump-rating.json 将 Envoy 的运行时配置 dump 出来之后你将看到一个长 6000 余行的配置文件。关于该配置文件的介绍请参考 Envoy v2 API 概览。 下图展示的是 Enovy 的配置。 图片 - Envoy 配置 Istio 会在为 Service Mesh 中的每个 Pod 注入 Sidecar 的时候同时为 Envoy 注入 Bootstrap 配置，其余的配置是通过 Pilot 下发的，注意整个数据平面即 Service Mesh 中的 Envoy 的动态配置应该是相同的。您也可以使用上面的命令检查其他 sidecar 的 Envoy 配置是否跟最上面的那个相同。 使用下面的命令检查 Service Mesh 中的所有有 Sidecar 注入的 Pod 中的 proxy 配置是否同步。 $ istioctl proxy-status PROXY CDS LDS EDS RDS PILOT VERSION details-v1-876bf485f-sx7df.default SYNCED SYNCED SYNCED (100%) SYNCED istio-pilot-5bf6d97f79-6lz4x 1.0.0 ... istioctl 这个命令行工具就像 kubectl 一样有很多神奇的魔法，通过它可以高效的管理 Istio 和 debug。 参考 kubernetes-vagrant-centos-cluster - github.com 点击关注【ServiceMesher】 微信公众号回复【加群】加入社区Copyright © servicemesher.com 2018-2019 all right reserved，powered by Gitbook Updated at 2019-03-13 17:48:25 "},"data-plane/envoy-proxy-config-deep-dive.html":{"url":"data-plane/envoy-proxy-config-deep-dive.html","title":"Envoy proxy 配置详解","summary":"本文详解了 Envoy proxy 的配置。","keywords":"","body":"Envoy proxy 配置详解 Istio envoy sidecar proxy 配置中包含以下四个部分。 bootstrap：Envoy proxy 启动时候加载的静态配置。 listeners：监听器配置，使用 LDS 下发。 clusters：集群配置，静态配置中包括 xds-grpc 和 zipkin 地址，动态配置使用 CDS 下发。 routes：路由配置，静态配置中包括了本地监听的服务的集群信息，其中引用了 cluster，动态配置使用 RDS 下发。 每个部分中都包含静态配置与动态配置，其中 bootstrap 配置又是在集群启动的时候通过 sidecar 启动参数注入的，配置文件在 /etc/istio/proxy/envoy-rev0.json。 Enovy 的配置 dump 出来后的结构如下图所示。 图片 - Envoy 配置 由于 bootstrap 中的配置是来自 Envoy 启动时加载的静态文件，主要配置了节点信息、tracing、admin 和统计信息收集等信息，这不是本文的重点，大家可以自行研究。 图片 - bootstrap 配置 上图是 bootstrap 的配置信息。 Bootstrap 是 Envoy 中配置的根本来源，Bootstrap 消息中有一个关键的概念，就是静态和动态资源的之间的区别。例如 Listener 或 Cluster 这些资源既可以从 static_resources 静态的获得也可以从 dynamic_resources 中配置的 LDS 或 CDS 之类的 xDS 服务获取。 Listener Listener 顾名思义，就是监听器，监听 IP 地址和端口，然后根据策略转发。 Listener 的特点 每个 Envoy 进程中可以有多个 Listener，Envoy 与 Listener 之间是一对多的关系。 每个 Listener 中可以配置一条 filter 链表（filter_chains），Envoy 会根据 filter 顺序执行过滤。 Listener 可以监听下游的端口，也可以接收来自其他 listener 的数据，形成链式处理。 filter 是可扩展的。 可以静态配置，也可以使用 LDS 动态配置。 目前只能监听 TCP，UDP 还未支持。 Listener 的数据结构 Listener 的数据结构如下，除了 name、address 和 filter_chains 为必须配置之外，其他都为可选的。 { \"name\": \"...\", \"address\": \"{...}\", \"filter_chains\": [], \"use_original_dst\": \"{...}\", \"per_connection_buffer_limit_bytes\": \"{...}\", \"metadata\": \"{...}\", \"drain_type\": \"...\", \"listener_filters\": [], \"transparent\": \"{...}\", \"freebind\": \"{...}\", \"socket_options\": [], \"tcp_fast_open_queue_length\": \"{...}\", \"bugfix_reverse_write_filter_order\": \"{...}\" } 下面是关于上述数据结构中的常用配置解析。 name：该 listener 的 UUID，唯一限定名，默认60个字符，例如 10.254.74.159_15011，可以使用命令参数指定长度限制。 address：监听的逻辑/物理地址和端口号，例如 \"address\": { \"socket_address\": { \"address\": \"10.254.74.159\", \"port_value\": 15011 } } filter_chains：这是一个列表，Envoy 中内置了一些通用的 filter，每种 filter 都有特定的数据结构，Enovy 会根据该配置顺序执行 filter。Envoy 中内置的 filter 有：envoy.client_ssl_auth、envoy.echo、enovy.http_connection_manager、envoy.mongo_proxy、envoy.rate_limit、enovy.redis_proxy、envoy.tcp_proxy、http_filters、thrift_filters等。这些 filter 可以单独使用也可以组合使用，还可以自定义扩展，例如使用 Istio 中的 EnvoyFilter 配置。 use_original_dst：这是一个布尔值，如果使用 iptables 重定向连接，则代理接收的端口可能与原始目的地址的端口不一样。当此标志设置为 true 时，Listener 将重定向的连接切换到与原始目的地址关联的 Listener。如果没有与原始目的地址关联的 Listener，则连接由接收它的 Listener 处理。默认为 false。注意：该参数将被废弃，请使用原始目的地址的 Listener filter 替代。该参数的主要用途是，Envoy 通过监听 15001 端口将应用的流量截取后再由其他 Listener 处理而不是直接转发出去，详情见 Virtual Listener。 关于 Listener 的详细介绍请参考 Envoy v2 API reference - listener。 Cluster Cluster 是指 Envoy 连接的一组逻辑相同的上游主机。Envoy 通过服务发现来发现 cluster 的成员。可以选择通过主动健康检查来确定集群成员的健康状态。Envoy 通过负载均衡策略决定将请求路由到 cluster 的哪个成员。 Cluster 的特点 一组逻辑上相同的主机构成一个 cluster。 可以在 cluster 中定义各种负载均衡策略。 新加入的 cluster 需要一个热身的过程才可以给路由引用，该过程是原子的，即在 cluster 热身之前对于 Envoy 及 Service Mesh 的其余部分来说是不可见的。 可以通过多种方式来配置 cluster，例如静态类型、严格限定 DNS、逻辑 DNS、EDS 等。 Cluster 的数据结构 Cluster 的数据结构如下，除了 name 字段，其他都是可选的。 { \"name\": \"...\", \"alt_stat_name\": \"...\", \"type\": \"...\", \"eds_cluster_config\": \"{...}\", \"connect_timeout\": \"{...}\", \"per_connection_buffer_limit_bytes\": \"{...}\", \"lb_policy\": \"...\", \"hosts\": [], \"load_assignment\": \"{...}\", \"health_checks\": [], \"max_requests_per_connection\": \"{...}\", \"circuit_breakers\": \"{...}\", \"tls_context\": \"{...}\", \"common_http_protocol_options\": \"{...}\", \"http_protocol_options\": \"{...}\", \"http2_protocol_options\": \"{...}\", \"extension_protocol_options\": \"{...}\", \"dns_refresh_rate\": \"{...}\", \"dns_lookup_family\": \"...\", \"dns_resolvers\": [], \"outlier_detection\": \"{...}\", \"cleanup_interval\": \"{...}\", \"upstream_bind_config\": \"{...}\", \"lb_subset_config\": \"{...}\", \"ring_hash_lb_config\": \"{...}\", \"original_dst_lb_config\": \"{...}\", \"least_request_lb_config\": \"{...}\", \"common_lb_config\": \"{...}\", \"transport_socket\": \"{...}\", \"metadata\": \"{...}\", \"protocol_selection\": \"...\", \"upstream_connection_options\": \"{...}\", \"close_connections_on_host_health_failure\": \"...\", \"drain_connections_on_host_removal\": \"...\" } 下面是关于上述数据结构中的常用配置解析。 name：如果你留意到作为 Sidecar 启动的 Envoy 的参数的会注意到 --max-obj-name-len 189，该选项用来用来指定 cluster 的名字，例如 inbound|9080||ratings.default.svc.cluster.local。该名字字符串由 | 分隔成四个部分，分别是 inbound 或 outbound 代表入向流量或出向流量、端口号、subcluster 名称、FQDN，其中 subcluster 名称将对应于 Istio DestinationRule 中配置的 subnet，如果是按照多版本按比例路由的话，该值可以是版本号。 type：即服务发现类型，支持的参数有 STATIC（缺省值）、STRICT_DNS、LOGICAL_DNS、EDS、ORIGINAL_DST。 hosts：这是个列表，配置负载均衡的 IP 地址和端口，只有使用了 STATIC、STRICT_DNS、LOGICAL_DNS 服务发现类型时才需要配置。 eds_cluster_config：如果使用 EDS 做服务发现，则需要配置该项目，其中包括的配置有 service_name 和 ads。 关于 Cluster 的详细介绍请参考 Envoy v2 API reference - cluster。 Route 我们在这里所说的路由指的是 HTTP 路由，这也使得 Envoy 可以用来处理网格边缘的流量。HTTP 路由转发是通过路由过滤器实现的。该过滤器的主要职能就是执行路由表中的指令。除了可以做重定向和转发，路由过滤器还需要处理重试、统计之类的任务。 HTTP 路由的特点 前缀和精确路径匹配规则。 可跨越多个上游集群进行基于权重/百分比的路由。 基于优先级的路由。 基于哈希策略的路由。 Route 的数据结构 { \"name\": \"...\", \"virtual_hosts\": [], \"internal_only_headers\": [], \"response_headers_to_add\": [], \"response_headers_to_remove\": [], \"request_headers_to_add\": [], \"request_headers_to_remove\": [], \"validate_clusters\": \"{...}\" } 下面是关于上述数据结构中的常用配置解析。 name：该名字跟 envoy.http_connection_manager filter 中的 http_filters.rds.route_config_name 一致，在 Istio Service Mesh 中为 Envoy 下发的配置中的 Route 是以监听的端口号作为名字，而同一个名字下面的 virtual_hosts 可以有多个值（数组形式）。 virtual_hosts：因为 VirtualHosts 是 Envoy 中引入的一个重要概念，我们在下文将详细说明 virtual_hosts 的数据结构。 validate_clusters：这是一个布尔值，用来设置开启使用 cluster manager 来检测路由表引用的 cluster 是否有效。如果是路由表是通过 route_config 静态配置的则该值默认设置为 true，如果是使用 rds 动态配置的话，则该值默认设置为 false。 关于 Route 的详细介绍请参考 Envoy v2 API reference - HTTP route configuration。 route.VirtualHost VirtualHost 即上文中 Route 配置中的 virtual_hosts，VirtualHost 是路由配置中的顶级元素。每个虚拟主机都有一个逻辑名称以及一组根据传入请求的 host header 路由到它的域。这允许单个 Listener 为多个顶级域路径树提供服务。基于域选择了虚拟主机后 Envoy 就会处理路由以查看要路由到哪个上游集群或是否执行重定向。 VirtualHost 的数据结构 下面是 VirtualHost 的数据结构，除了 name 和 domains 是必须配置项外，其他皆为可选项。 { \"name\": \"...\", \"domains\": [], \"routes\": [], \"require_tls\": \"...\", \"virtual_clusters\": [], \"rate_limits\": [], \"request_headers_to_add\": [], \"request_headers_to_remove\": [], \"response_headers_to_add\": [], \"response_headers_to_remove\": [], \"cors\": \"{...}\", \"per_filter_config\": \"{...}\", \"include_request_attempt_count\": \"...\" } 下面是关于上述数据结构中的常用配置解析。 name：该 VirtualHost 的名字，一般是 FQDN 加端口，如 details.default.svc.cluster.local:9080。 domains：这是个用来匹配 VirtualHost 的域名（host/authority header）列表，也可以使用通配符，但是通配符不能匹配空字符，除了仅使用 * 作为 domains，注意列表中的值不能重复和存在交集，只要有一条 domain 被匹配上了，就会执行路由。Istio 会为该值配置所有地址解析形式，包括 IP 地址、FQDN 和短域名等。 routes：针对入口流量的有序路由列表，第一个匹配上的路由将被执行。我们在下文将详细说明 route 的数据结构。 下面是一个实际的 VirtualHost 的例子，该配置来自 Bookinfo 应用的 details 应用的 Sidecar 服务。 { \"name\": \"details.default.svc.cluster.local:9080\", \"domains\": [ \"details.default.svc.cluster.local\", \"details.default.svc.cluster.local:9080\", \"details\", \"details:9080\", \"details.default.svc.cluster\", \"details.default.svc.cluster:9080\", \"details.default.svc\", \"details.default.svc:9080\", \"details.default\", \"details.default:9080\", \"10.254.4.113\", \"10.254.4.113:9080\" ], \"routes\": [ { \"match\": { \"prefix\": \"/\" }, \"route\": { \"cluster\": \"outbound|9080||details.default.svc.cluster.local\", \"timeout\": \"0s\", \"max_grpc_timeout\": \"0s\" }, \"decorator\": { \"operation\": \"details.default.svc.cluster.local:9080/*\" }, \"per_filter_config\": { \"mixer\": { \"forward_attributes\": { \"attributes\": { \"destination.service.uid\": { \"string_value\": \"istio://default/services/details\" }, \"destination.service.host\": { \"string_value\": \"details.default.svc.cluster.local\" }, \"destination.service.namespace\": { \"string_value\": \"default\" }, \"destination.service.name\": { \"string_value\": \"details\" }, \"destination.service\": { \"string_value\": \"details.default.svc.cluster.local\" } } }, \"mixer_attributes\": { \"attributes\": { \"destination.service.host\": { \"string_value\": \"details.default.svc.cluster.local\" }, \"destination.service.uid\": { \"string_value\": \"istio://default/services/details\" }, \"destination.service.name\": { \"string_value\": \"details\" }, \"destination.service.namespace\": { \"string_value\": \"default\" }, \"destination.service\": { \"string_value\": \"details.default.svc.cluster.local\" } } }, \"disable_check_calls\": true } } } ] } 关于 route.VirtualHost 的详细介绍请参考 Envoy v2 API reference - route.VirtualHost。 route.Route 路由既是如何匹配请求的规范，也是对下一步做什么的指示（例如，redirect、forward、rewrite等）。 route.Route 的数据结构 下面是是 route.Route 的数据结构，除了 match 之外其余都是可选的。 { \"match\": \"{...}\", \"route\": \"{...}\", \"redirect\": \"{...}\", \"direct_response\": \"{...}\", \"metadata\": \"{...}\", \"decorator\": \"{...}\", \"per_filter_config\": \"{...}\", \"request_headers_to_add\": [], \"request_headers_to_remove\": [], \"response_headers_to_add\": [], \"response_headers_to_remove\": [] } 下面是关于上述数据结构中的常用配置解析。 match：路由匹配参数。例如 URL prefix（前缀）、path（URL 的完整路径）、regex（规则表达式）等。 route：这里面配置路由的行为，可以是 route、redirect 和 direct_response，不过这里面没有专门的一个配置项用来配置以上三种行为，而是根据实际填充的配置项来确定的。例如在此处添加 cluster 配置则暗示路由动作为”route“，表示将流量路由到该 cluster。详情请参考 route.RouteAction。 decorator：被匹配的路由的修饰符，表示被匹配的虚拟主机和 URL。该配置里有且只有一个必须配置的项 operation，例如 details.default.svc.cluster.local:9080/*。 per_filter_config：这是一个 map 类型，per_filter_config 字段可用于为 filter 提供特定路由的配置。Map 的 key 应与 filleter 名称匹配，例如用于 HTTP buffer filter 的 envoy.buffer。该字段是特定于 filter 的，详情请参考 HTTP filter。 关于 route.Route 的详细介绍请参考 Envoy v2 API reference - route.Route。 参考 Envoy v2 API 概览 - servicemesher.com 监听器发现服务（LDS）- servicemesher.com 路由发现服务（RDS）- servicemesher.com 集群发现服务（CDS）- servicemesher.com 点击关注【ServiceMesher】 微信公众号回复【加群】加入社区Copyright © servicemesher.com 2018-2019 all right reserved，powered by Gitbook Updated at 2019-03-10 16:34:10 "},"data-plane/envoy-api.html":{"url":"data-plane/envoy-api.html","title":"Envoy API","summary":"本文介绍了 Envoy API。","keywords":"","body":"Envoy API Envoy 提供了如下的 API： CDS（Cluster Discovery Service）：集群发现服务 EDS（Endpoint Discovery Service）：端点发现服务 HDS（Health Discovery Service）：健康发现服务 LDS（Listener Discovery Service）：监听器发现服务 MS（Metric Service）：将 metric 推送到远端服务器 RLS（Rate Limit Service）：速率限制服务 RDS（Route Discovery Service）：路由发现服务 SDS（Secret Discovery Service）：秘钥发现服务 所有名称以 DS 结尾的服务统称为 xDS。 本书中仅讨论 v2 版本的 API，因为 Envoy 仍在不断开发和完善中，随着版本迭代也有可能新增一些 API，本章的重点在于 xDS 协议，关于 Envoy 的 API 的更多信息请参考 Envoy v2 APIs for developers。 Envoy xDS 协议 Envoy xDS 为 Istio 控制平面与控制平面通信的基本协议，只要代理支持该协议表达形式就可以创建自己的 Sidecar 来替换 Envoy。这一章中将带大家了解 Envoy xDS。 Envoy 是 Istio Service Mesh 中默认的 Sidecar，Istio 在 Enovy 的基础上按照 Envoy 的 xDS 协议扩展了其控制平面，在讲到 Envoy xDS 协议之前还需要我们先熟悉下 Envoy 的基本术语。下面列举了 Envoy 里的基本术语及其数据结构解析，关于 Envoy 的详细介绍请参考 Envoy 官方文档，至于 Envoy 在 Service Mesh（不仅限于 Istio） 中是如何作为转发代理工作的请参考网易云刘超的这篇深入解读 Service Mesh 背后的技术细节 以及理解 Istio Service Mesh 中 Envoy 代理 Sidecar 注入及流量劫持，本文引用其中的一些观点，详细内容不再赘述。 图片 - Envoy proxy 架构图 关于 xDS 的版本 有一点需要大家注意，就是 Envoy 的 API 有 v1 和 v2 两个版本，从 Envoy 1.5.0 起 v2 API 就已经生产就绪了，为了能够让用户顺利的向 v2 版本的 API 过度，Envoy 启动的时候设置了一个 --v2-config-only 的标志，Enovy 不同版本对 v1/v2 API 的支持详情请参考 Envoy v1 配置废弃时间表。 Envoy 的作者 Matt Klein 在 Service Mesh 中的通用数据平面 API 设计这篇文章中说明了 Envoy API v1 的历史及其缺点，还有 v2 的引入。v2 API 是 v1 的演进，而不是革命，它是 v1 功能的超集。 在 Istio 1.0 及以上版本中使用的是 Envoy 1.8.0-dev 版本，其支持 v2 的 API，同时在 Envoy 作为 Sidecar proxy 启动的使用使用了例如下面的命令： $ /usr/local/bin/envoy -c /etc/istio/proxy/envoy-rev0.json --restart-epoch 0 --drain-time-s 45 --parent-shutdown-time-s 60 --service-cluster ratings --service-node sidecar~172.33.14.2~ratings-v1-8558d4458d-ld8x9.default~default.svc.cluster.local --max-obj-name-len 189 --allow-unknown-fields -l warn --v2-config-only 上面是都 Bookinfo 示例中的 rating pod 中的 sidecar 启动的分析，可以看到其中指定了 --v2-config-only，表明 Istio 1.0+ 只支持 xDS v2 的 API。 REST-JSON & gPRC API 单个的基本 xDS 订阅服务，如 CDS、EDS、LDS、RDS、SDS 同时支持 REST-JSON 和 gRPC API 配置。高级 API，如 HDS、ADS 和 EDS 多维 LB 仅支持 gRPC。这是为了避免将复杂的双向流语义映射到 REST。详见 Envoy v2 APIs for developers。 参考 Service Mesh 中的通用数据平面 API 设计 - servicemesher.com Envoy v2 APIs for developers - github.com 点击关注【ServiceMesher】 微信公众号回复【加群】加入社区Copyright © servicemesher.com 2018-2019 all right reserved，powered by Gitbook Updated at 2019-03-13 17:48:25 "},"data-plane/envoy-xds-protocol.html":{"url":"data-plane/envoy-xds-protocol.html","title":"xDS 协议解析","summary":"本文是 xDS 协议的解析，译自 Matt Klein 的博客。","keywords":"","body":"xDS 协议解析 本文译自 xDS REST and gRPC protocol，译者：狄卫华，审校：宋净超 Envoy 通过查询文件或管理服务器来动态发现资源。概括地讲，对应的发现服务及其相应的 API 被称作 xDS。Envoy 通过订阅（subscription）方式来获取资源，如监控指定路径下的文件、启动 gRPC 流或轮询 REST-JSON URL。后两种方式会发送 DiscoveryRequest 请求消息，发现的对应资源则包含在响应消息 DiscoveryResponse 中。下面，我们将具体讨论每种订阅类型。 文件订阅 发现动态资源的最简单方式就是将其保存于文件，并将路径配置在 ConfigSource 中的 path 参数中。Envoy 使用 inotify（Mac OS X 上为 kqueue）来监控文件的变化，在文件被更新时，Envoy 读取保存的 DiscoveryResponse 数据进行解析，数据格式可以为二进制 protobuf、JSON、YAML 和协议文本等。 译者注：core.ConfigSource 配置格式如下： { \"path\": \"...\", \"api_config_source\": \"{...}\", \"ads\": \"{...}\" } 文件订阅方式可提供统计数据和日志信息，但是缺少 ACK/NACK 更新的机制。如果更新的配置被拒绝，xDS API 则继续使用最后一个的有效配置。 gRPC 流式订阅 单资源类型发现 每个 xDS API 可以单独配置 ApiConfigSource，指向对应的上游管理服务器的集群地址。每个 xDS 资源类型会启动一个独立的双向 gRPC 流，可能对应不同的管理服务器。API 交付方式采用最终一致性。可以参考后续聚合服务发现（ADS） 章节来了解必要的显式控制序列。 译者注：core.ApiConfigSource 配置格式如下： { \"api_type\": \"...\", \"cluster_names\": [], \"grpc_services\": [], \"refresh_delay\": \"{...}\", \"request_timeout\": \"{...}\" } 类型 URL 每个 xDS API 都与给定的资源的类型存在 1:1 对应。关系如下： LDS： envoy.api.v2.Listener RDS： envoy.api.v2.RouteConfiguration CDS： envoy.api.v2.Cluster EDS： envoy.api.v2.ClusterLoadAssignment SDS：envoy.api.v2.Auth.Secret 类型 URL 的概念如下所示，其采用 type.googleapis.com/ 的形式，例如 CDS 对应于 type.googleapis.com/envoy.api.v2.Cluster。在 Envoy 的请求和管理服务器的响应中，都包括了资源类型 URL。 ACK/NACK 和版本 每个 Envoy 流以 DiscoveryRequest 开始，包括了列表订阅的资源、订阅资源对应的类型 URL、节点标识符和空的 version_info。EDS 请求示例如下： version_info: node: { id: envoy } resource_names: - foo - bar type_url: type.googleapis.com/envoy.api.v2.ClusterLoadAssignment response_nonce: 管理服务器可立刻或等待资源就绪时发送 DiscoveryResponse作为响应，示例如下： version_info: X resources: - foo ClusterLoadAssignment proto encoding - bar ClusterLoadAssignment proto encoding type_url: type.googleapis.com/envoy.api.v2.ClusterLoadAssignment nonce: A Envoy 在处理 DiscoveryResponse 响应后，将通过流发送一个新的请求，请求包含应用成功的最后一个版本号和管理服务器提供的 nonce。如果本次更新已成功应用，则 version_info 的值设置为 X，如下序列图所示： 图片 - ACK 后的版本更新 在此序列图及后续中，将统一使用以下缩写格式： DiscoveryRequest： (V=version_info，R=resource_names，N=response_nonce，T=type_url) DiscoveryResponse： (V=version_info，R=resources，N=nonce，T=type_url) 译者注：在信息安全中，Nonce是一个在加密通信只能使用一次的数字。在认证协议中，它往往是一个随机或伪随机数，以避免重放攻击。Nonce也用于流密码以确保安全。如果需要使用相同的密钥加密一个以上的消息，就需要Nonce来确保不同的消息与该密钥加密的密钥流不同。（引用自维基百科）在本文中nonce是每次更新的数据包的唯一标识。 版本为 Envoy 和管理服务器提供了共享当前应用配置的概念和通过 ACK/NACK 来进行配置更新的机制。如果 Envoy 拒绝配置更新 X，则回复 error_detail 及前一个的版本号，在当前情况下为空的初始版本号，error_detail 包含了有关错误的更加详细的信息： 图片 - NACK 无版本更新 后续，API 更新可能会在新版本 Y 上成功： 图片 - ACK 紧接着 NACK 每个流都有自己的版本概念，但不存在跨资源类型的共享版本。在不使用 ADS 的情况下，每个资源类型可能具有不同的版本，因为 Envoy API 允许指向不同的 EDS/RDS 资源配置并对应不同的 ConfigSources。 何时发送更新 管理服务器应该只向 Envoy 客户端发送上次 DiscoveryResponse 后更新过的资源。Envoy 则会根据接受或拒绝 DiscoveryResponse 的情况，立即回复包含 ACK/NACK 的 DiscoveryRequest 请求。如果管理服务器每次发送相同的资源集结果，而不是根据其更新情况，则会导致 Envoy 和管理服务器通讯效率大打折扣。 在同一个流中，新的 DiscoveryRequests 将取代此前具有相同的资源类型 DiscoveryRequest 请求。这意味着管理服务器只需要响应给定资源类型最新的 DiscoveryRequest 请求即可。 资源提示 DiscoveryRequest 中的 resource_names 信息作为资源提示出现。一些资源类型，例如 Cluster 和 Listener 将使用一个空的 resource_names，因为 Envoy 需要获取管理服务器对应于节点标识的所有 Cluster（CDS）和 Listener（LDS）。对于其他资源类型，如 RouteConfigurations（RDS）和 ClusterLoadAssignments（EDS），则遵循此前的 CDS/LDS 更新，Envoy 能够明确地枚举这些资源。 LDS/CDS 资源提示信息将始终为空，并且期望管理服务器的每个响应都提供 LDS/CDS 资源的完整状态。缺席的 Listener 或 Cluster 将被删除。 对于 EDS/RDS，管理服务器并不需要为每个请求的资源进行响应，而且还可能提供额外未请求的资源。resource_names 只是一个提示。Envoy 将默默地忽略返回的多余资源。如果请求的资源中缺少相应的 RDS 或 EDS 更新，Envoy 将保留对应资源的最后的值。管理服务器可能会依据 DiscoveryRequest 中 node 标识推断其所需的 EDS/RDS 资源，在这种情况下，提示信息可能会被丢弃。从相应的角度来看，空的 EDS/RDS DiscoveryResponse 响应实际上是表明在 Envoy 中为一个空的资源。 当 Listener 或 Cluster 被删除时，其对应的 EDS 和 RDS 资源也需要在 Envoy 实例中删除。为使 EDS 资源被 Envoy 已知或跟踪，就必须存在应用过的 Cluster 定义（如通过 CDS 获取）。RDS 和 Listeners 之间存在类似的关系（如通过 LDS 获取）。 对于 EDS/RDS ，Envoy 可以为每个给定类型的资源生成不同的流（如每个 ConfigSource 都有自己的上游管理服务器的集群）或当指定资源类型的请求发送到同一个管理服务器的时候，允许将多个资源请求组合在一起发送。虽然可以单个实现，但管理服务器应具备处理每个给定资源类型中对单个或多个 resource_names 请求的能力。下面的两个序列图对于获取两个 EDS 资源都是有效的 {foo，bar}： 资源更新 如上所述，Envoy 可能会更新 DiscoveryRequest 中出现的 resource_names 列表，其中 DiscoveryRequest 是用来 ACK/NACK 管理服务器的特定的 DiscoveryResponse 。此外，Envoy 后续可能会发送额外的 DiscoveryRequests ，用于在特定 version_info 上使用新的资源提示来更新管理服务器。例如，如果 Envoy 在 EDS 版本 X 时仅知道集群 foo，但在随后收到的 CDS 更新时额外获取了集群 bar ，它可能会为版本 X 发出额外的 DiscoveryRequest 请求，并将 {foo，bar} 作为请求的 resource_names 。 图片 - CDS 响应导致 EDS 资源更新 这里可能会出现竞争状况；如果 Envoy 在版本 X 上发布了资源提示更新请求，但在管理服务器处理该请求之前发送了新的版本号为 Y 的响应，针对 version_info 为 X 的版本，资源提示更新可能会被解释为拒绝 Y 。为避免这种情况，通过使用管理服务器提供的 nonce，Envoy 可用来保证每个 DiscoveryRequest 对应到相应的 DiscoveryResponse ： 图片 - EDS 更新速率激发 nonces 管理服务器不应该为含有过期 nonce 的 DiscoveryRequest 发送 DiscoveryResponse 响应。在向 Envoy 发送的 DiscoveryResponse 中包含了的新 nonce ，则此前的 nonce 将过期。在资源新版本就绪之前，管理服务器不需要向 Envoy 发送更新。同版本的早期请求将会过期。在新版本就绪时，管理服务器可能会处理同一个版本号的多个 DiscoveryRequests请求。 图片 - 请求变的陈旧 上述资源更新序列表明 Envoy 并不能期待其发出的每个 DiscoveryRequest 都得到 DiscoveryResponse 响应。 最终一致性考虑 由于 Envoy 的 xDS API 采用最终一致性，因此在更新期间可能导致流量被丢弃。例如，如果通过 CDS/EDS 仅获取到了集群 X，而且 RouteConfiguration 引用了集群 X；在 CDS/EDS 更新集群 Y 配置之前，如果将 RouteConfiguration 将引用的集群调整为 Y ，那么流量将被吸入黑洞而丢弃，直至集群 Y 被 Envoy 实例获取。 对某些应用程序，可接受临时的流量丢弃，客户端重试或其他 Envoy sidecar 会掩盖流量丢弃。那些对流量丢弃不能容忍的场景，可以通过以下方式避免流量丢失，CDS/EDS 更新同时携带 X 和 Y ，然后发送 RDS 更新从 X 切换到 Y ，此后发送丢弃 X 的 CDS/EDS 更新。 一般来说，为避免流量丢弃，更新的顺序应该遵循 make before break 模型，其中 必须始终先推送 CDS 更新（如果有）。 EDS 更新（如果有）必须在相应集群的 CDS 更新后到达。 LDS 更新必须在相应的 CDS/EDS 更新后到达。 与新添加的监听器相关的 RDS 更新必须在最后到达。 最后，删除过期的 CDS 集群和相关的 EDS 端点（不再被引用的端点）。 如果没有新的集群/路由/监听器或者允许更新时临时流量丢失的情况下，可以独立推送 xDS 更新。请注意，在 LDS 更新的情况下，监听器须在接收流量之前被预热，例如如其配置了依赖的路由，则先需先从 RDS 进行获取。添加/删除/更新集群信息时，集群也需要进行预热。另一方面，如果管理平面确保路由更新时所引用的集群已经准备就绪，路由可以不用预热。 聚合服务发现（ADS） 当管理服务器进行资源分发时，通过上述保证交互顺序的方式来避免流量丢弃是一项很有挑战的工作。ADS 允许单一管理服务器通过单个 gRPC 流，提供所有的 API 更新。配合仔细规划的更新顺序，ADS 可规避更新过程中流量丢失。使用 ADS，在单个流上可通过类型 URL 来进行复用多个独立的 DiscoveryRequest/DiscoveryResponse 序列。对于任何给定类型的 URL，以上 DiscoveryRequest 和 DiscoveryResponse 消息序列都适用。 更新序列可能如下所示： 图片 - EDS/CDS 在一个 ADS 流上多路复用 每个 Envoy 实例可使用单独的 ADS 流。 最小化 ADS 配置的 bootstrap.yaml 片段示例如下： node: id: dynamic_resources: cds_config: {ads: {}} lds_config: {ads: {}} ads_config: api_type: GRPC grpc_services: envoy_grpc: cluster_name: ads_cluster static_resources: clusters: - name: ads_cluster connect_timeout: { seconds: 5 } type: STATIC hosts: - socket_address: address: port_value: lb_policy: ROUND_ROBIN http2_protocol_options: {} admin: ... 增量 xDS 增量 xDS 是可用于允许的 ADS、CDS 和 RDS 单独 xDS 端点： xDS 客户端对跟踪资源列表进行增量更新。这支持 Envoy 按需/惰性地请求额外资源。例如，当与未知集群相对应的请求到达时，可能会发生这种情况。 xDS 服务器可以增量更新客户端上的资源。这支持 xDS 资源可伸缩性的目标。管理服务器只需交付更改的单个集群，而不是在修改单个集群时交付所有上万个集群。 xDS 增量会话始终位于 gRPC 双向流的上下文中。这允许 xDS 服务器能够跟踪到连接的 xDS 客户端的状态。xDS REST 版本不支持增量。 在增量 xDS 中，nonce 字段是必需的，用于匹配 IncrementalDiscoveryResponse 关联的 ACK 或 NACK IncrementalDiscoveryRequest。可选地，存在响应消息级别的 system_version_info，但仅用于调试目的。 IncrementalDiscoveryRequest 可在以下 3 种情况下发送： xDS 双向 gRPC 流的初始消息。 作为对先前的 IncrementalDiscoveryResponse 的 ACK 或 NACK 响应。在这种情况下，response_nonce 被设置为响应中的 nonce 值。ACK 或 NACK 由可由 error_detail 字段是否出现来区分。 客户端自发的 IncrementalDiscoveryRequest。此场景下可以采用动态添加或删除被跟踪的 resource_names 集。这种场景下，必须忽略 response_nonce。 在第一个示例中，客户端连接并接收它的第一个更新并 ACK。第二次更新失败，客户端发送 NACK 拒绝更新。xDS客户端后续会自发地请求 “wc” 相关资源。 图片 - 增量 session 示例 在重新连接时，支持增量的 xDS 客户端可能会告诉服务器其已知资源从而避免通过网络重新发送它们。 图片 - 增量重连示例 REST-JSON 轮询订阅 单个 xDS API 可对 REST 端点进行同步（长）轮询。除了无持久流与管理服务器交互外，消息顺序与上述相似。在任何时间点，只存在一个未完成的请求，因此响应消息中的 nonce 在 REST-JSON 中是可选的。DiscoveryRequest 和 DiscoveryResponse 的消息编码遵循 JSON 变换 proto3 规范。ADS 不支持 REST-JSON 轮询。 当轮询期间设置为较小的值时，则可以等同于长轮询，这时要求避免发送 DiscoveryResponse，除非对请求的资源发生了更改。 参考 xDS REST and gRPC protocol - github.com 点击关注【ServiceMesher】 微信公众号回复【加群】加入社区Copyright © servicemesher.com 2018-2019 all right reserved，powered by Gitbook Updated at 2019-03-10 16:34:28 "},"data-plane/envoy-lds.html":{"url":"data-plane/envoy-lds.html","title":"LDS（监听器发现服务）","summary":"本文是 xDS 协议中 LDS 的解析，译自 Envoy 官方文档。","keywords":"","body":"LDS（监听器发现服务） Listener 发现服务（LDS）是一个可选的 API，Envoy 将调用它来动态获取 Listener。Envoy 将协调 API 响应，并根据需要添加、修改或删除已知的 Listener。 Listener 更新的语义如下： 每个 Listener 必须有一个独特的名字。如果没有提供名称，Envoy 将创建一个 UUID。要动态更新的 Listener ，管理服务必须提供 Listener 的唯一名称。 当一个 Listener 被添加，在参与连接处理之前，会先进入“预热”阶段。例如，如果 Listener 引用 RDS 配置，那么在 Listener 迁移到 “active” 之前，将会解析并提取该配置。 Listener 一旦创建，实际上就会保持不变。因此，更新 Listener 时，会创建一个全新的 Listener （使用相同的侦听套接字）。新增加的监听者都会通过上面所描述的相同“预热”过程。 当更新或删除 Listener 时，旧的 Listener 将被置于 “draining（逐出）” 状态，就像整个服务重新启动时一样。Listener 移除之后，该 Listener 所拥有的连接，经过一段时间优雅地关闭（如果可能的话）剩余的连接。逐出时间通过 --drain-time-s 选项设置。 注意 Envoy 从 1.9 版本开始已不再支持 v1 API。 v2 LDS API 统计 LDS 的统计树是以 listener_manager.lds 为根，统计如下： 名字 类型 描述 config_reload Counter 因配置不同而导致配置重新加载的总次数 update_attempt Counter 尝试调用配置加载 API 的总次数 update_success Counter 调用配置加载 API 成功的总次数 update_failure Counter 调用配置加载 API 因网络错误的失败总数 update_rejected Counter 调用配置加载 API 因 schema/验证错误的失败总次数 version Gauge 来自上次成功调用配置加载API的内容哈希 control_plane.connected_state Gauge 布尔值，用来表示与管理服务器的连接状态，1表示已连接，0表示断开连接 参考 Listener discovery service(LDS) - envoyproxy.io 监听器发现服务（LDS）- servicemesher.com 点击关注【ServiceMesher】 微信公众号回复【加群】加入社区Copyright © servicemesher.com 2018-2019 all right reserved，powered by Gitbook Updated at 2019-03-10 16:34:37 "},"data-plane/envoy-rds.html":{"url":"data-plane/envoy-rds.html","title":"RDS（路由发现服务）","summary":"本文是 xDS 协议中 RDS 的解析，译自 Envoy 官方文档。","keywords":"","body":"RDS（路由发现服务） 路由发现服务（RDS）是 Envoy 里面的一个可选 API，用于动态获取路由配置。路由配置包括 HTTP header 修改、虚拟主机以及每个虚拟主机中包含的单个路由规则配置。每个 HTTP 连接管理器都可以通过 API 独立地获取自身的路由配置。 v2 API 参考 注意：Envoy 从 1.9 版本开始已不再支持 v1 API。 统计 RDS 的统计树以 http..rds..*.为根，route_config_name名称中的任何:字符在统计树中被替换为_。统计树包含以下统计信息： 名字 类型 描述 config_reload Counter 因配置不同而导致配置重新加载的总次数 update_attempt Counter 尝试调用配置加载 API 的总次数 update_success Counter 调用配置加载 API 成功的总次数 update_failure Counter 调用配置加载 API 因网络错误的失败总数 update_rejected Counter 调用配置加载 API 因 schema/验证错误的失败总次数 version Gauge 来自上次成功调用配置加载API的内容哈希 参考 Route discovery service(RDS) - envoyproxy.io 点击关注【ServiceMesher】 微信公众号回复【加群】加入社区Copyright © servicemesher.com 2018-2019 all right reserved，powered by Gitbook Updated at 2019-03-10 16:34:47 "},"data-plane/envoy-cds.html":{"url":"data-plane/envoy-cds.html","title":"CDS（集群发现服务）","summary":"本文是 xDS 协议中 CDS 的解析，译自 Envoy 官方文档。","keywords":"","body":"CDS（集群发现服务） 集群发现服务（CDS）是一个可选的 API，Envoy 将调用该 API 来动态获取 cluster manager 的成员。Envoy 还将根据 API 响应协调集群管理，根据需要完成添加、修改或删除已知的集群。 关于 Envoy 是如何通过 CDS 从 pilot-discovery 服务中获取的 cluster 配置，请参考 Service Mesh深度学习系列part3—istio源码分析之pilot-discovery模块分析（续）一文中的 CDS 服务部分。 注意 在 Envoy 配置中静态定义的 cluster 不能通过 CDS API 进行修改或删除。 Envoy 从 1.9 版本开始已不再支持 v1 API。 v2 CDS API 统计 CDS 的统计树以 cluster_manager.cds. 为根，统计如下： 名字 类型 描述 config_reload Counter 因配置不同而导致配置重新加载的总次数 update_attempt Counter 尝试调用配置加载 API 的总次数 update_success Counter 调用配置加载 API 成功的总次数 update_failure Counter 调用配置加载 API 因网络错误的失败总数 update_rejected Counter 调用配置加载 API 因 schema/验证错误的失败总次数 version Gauge 来自上次成功调用配置加载API的内容哈希 control_plane.connected_state Gauge 布尔值，用来表示与管理服务器的连接状态，1表示已连接，0表示断开连接 参考 Service Mesh深度学习系列part3—istio源码分析之pilot-discovery模块分析（续）- servicemesher.com Cluster discovery service - envoyproxy.io 点击关注【ServiceMesher】 微信公众号回复【加群】加入社区Copyright © servicemesher.com 2018-2019 all right reserved，powered by Gitbook Updated at 2019-03-10 16:34:58 "},"data-plane/envoy-eds.html":{"url":"data-plane/envoy-eds.html","title":"EDS（端点发现服务）","summary":"本文是 xDS 协议中 EDS 的解析，译自 Envoy 官方文档。","keywords":"","body":"EDS（端点发现服务） EDS 只是 Envoy 中众多的服务发现方式的一种。要想了解 EDS 首先我们需要先知道什么是 Endpoint。 Endpoint Endpoint 即上游主机标识。它的数据结构如下： { \"address\": \"{...}\", \"health_check_config\": \"{...}\" } 其中包括端点的地址和健康检查配置。详情请参考 endpoint.Endpoint。 终端发现服务（EDS）是一个基于 gRPC 或 REST-JSON API 服务器的 xDS 管理服务，在 Envoy 中用来获取集群成员。集群成员在 Envoy 的术语中被称为“终端”。对于每个集群，Envoy 都会通过发现服务来获取成员的终端。由于以下几个原因，EDS 是首选的服务发现机制： Envoy 对每个上游主机都有明确的了解（与通过 DNS 解析的负载均衡进行路由相比而言），并可以做出更智能的负载均衡决策。 在每个主机的发现 API 响应中携带的额外属性通知 Envoy 负载均衡权重、金丝雀状态、区域等。这些附加属性在负载均衡、统计信息收集等过程中会被 Envoy 网格全局使用。 Envoy 提供了 Java 和 Go 语言版本的 EDS 和其他发现服务的参考 gRPC 实现。 通常，主动健康检查与最终一致的服务发现服务数据结合使用，以进行负载均衡和路由决策。 参考 Service Mesh深度学习系列part3—istio源码分析之pilot-discovery模块分析（续）- servicemesher.com EDS - envoyproxy.io endpoint.Endpoint - envoyproxy.io 点击关注【ServiceMesher】 微信公众号回复【加群】加入社区Copyright © servicemesher.com 2018-2019 all right reserved，powered by Gitbook Updated at 2019-03-10 16:37:58 "},"data-plane/envoy-sds.html":{"url":"data-plane/envoy-sds.html","title":"SDS（秘钥发现服务）","summary":"本文是 xDS 协议中 SDS 的解析，译自 Envoy 官方文档。","keywords":"","body":"SDS（秘钥发现服务） SDS（秘钥发现服务）是 Envoy 1.8.0 版本起开始引入的服务。可以在 bootstrap.static_resource 的 secrets 配置中为 Envoy 指定 TLS 证书（secret）。也可以通过秘钥发现服务（SDS）远程获取。Istio 预计将在 1.1 版本中支持 SDS。 SDS 带来的最大的好处就是简化证书管理。要是没有该功能的话，我们就必须使用 Kubernetes 中的 secret 资源创建证书，然后把证书挂载到代理容器中。如果证书过期，还需要更新 secret 和需要重新部署代理容器。使用 SDS，中央 SDS 服务器将证书推送到所有 Envoy 实例上。如果证书过期，服务器只需将新证书推送到 Envoy 实例，Envoy 可以立即使用新证书而无需重新部署。 如果 listener server 需要从远程获取证书，则 listener server 不会被标记为 active 状态，在获取证书之前不会打开其端口。如果 Envoy 由于连接失败或错误的响应数据而无法获取证书，则 listener server 将被标记为 active，并且打开端口，但是将重置与端口的连接。 上游集群的处理方式类似，如果需要通过 SDS 从远程获取集群客户端证书，则不会将其标记为 active 状态，在获得证书之前也它不会被使用。如果 Envoy 由于连接失败或错误的响应数据而无法获取证书，则集群将被标记为 active，可以处理请求，但路由到该集群的请求都将被拒绝。 使用 SDS 的静态集群需定义 SDS 集群（除非使用不需要集群的 Google gRPC），则必须在使用静态集群之前定义 SDS 集群。 Envoy 代理和 SDS 服务器之间的连接必须是安全的。可以在同一主机上运行 SDS 服务器，使用 Unix Domain Socket 进行连接。否则，需要代理和 SDS 服务器之间的 mTLS。在这种情况下，必须静态配置 SDS 连接的客户端证书。 SDS Server SDS server 需要实现 SecretDiscoveryService 这个 gRPC 服务。遵循与其他 xDS 相同的协议。 SDS 配置 SDS 支持静态配置也支持动态配置。 静态配置 可以在static_resources 的 secrets 中配置 TLS 证书。 动态配置 从远程 SDS server 获取 secret。 通过 Unix Domain Socket 访问 gRPC SDS server。 通过 UDS 访问 gRPC SDS server。 通过 Envoy gRPC 访问 SDS server。 配置详情请参考 Envoy 官方文档。 参考 Secret discovery service (SDS) - envoyproxy.io 点击关注【ServiceMesher】 微信公众号回复【加群】加入社区Copyright © servicemesher.com 2018-2019 all right reserved，powered by Gitbook Updated at 2019-03-10 16:38:15 "},"data-plane/envoy-ads.html":{"url":"data-plane/envoy-ads.html","title":"ADS（聚合发现服务）","summary":"本文是 xDS 协议中 ADS 的解析，译自 Envoy 官方文档。","keywords":"","body":"ADS（聚合发现服务） 虽然 Envoy 本质上采用了最终一致性模型，但 ADS 提供了对 API 更新推送进行排序的机会，并确保单个管理服务器对 Envoy 节点的 API 更新具有亲和力。ADS 允许管理服务器在单个双向 gRPC 流上传递一个或多个 API 及其资源。否则，一些 API（如 RDS 和 EDS）可能需要管理多个流并连接到不同的管理服务器。 ADS 通过适当得排序 xDS 可以无中断的更新 Enovy 的配置。例如，假设 foo.com 已映射到集群 X。我们希望将路由表中将该映射更改为在集群 Y。为此，必须首先提供 X、Y 这两个集群的 CDS/EDS 更新。 如果没有 ADS，CDS/EDS/RDS 流可能指向不同的管理服务器，或者位于需要协调的不同 gRPC流连接的同一管理服务器上。EDS 资源请求可以跨两个不同的流分开，一个用于 X，一个用于 Y。ADS 将这些流合并到单个流和单个管理服务器，从而无需分布式同步就可以正确地对更新进行排序。使用 ADS，管理服务器将在单个流上提供 CDS、EDS 和 RDS 更新。 ADS 仅适用于 gRPC 流（非REST），本文档对此进行了更全面的描述。 参考 Aggregated Discovery Service - envoyproxy.io 点击关注【ServiceMesher】 微信公众号回复【加群】加入社区Copyright © servicemesher.com 2018-2019 all right reserved，powered by Gitbook Updated at 2019-04-03 20:13:45 "},"data-plane/envoy-hds.html":{"url":"data-plane/envoy-hds.html","title":"HDS（健康发现服务）","summary":"本文是 xDS 协议中 HDS 的解析，译自 Envoy 官方文档。","keywords":"","body":"HDS（健康发现服务） HDS（健康发现服务）支持管理服务器对其管理的 Envoy 实例进行高效的端点健康发现。单个 Envoy 实例通常会收到 HDS 指令，以检查所有端点的子集（subset）。运行状况检查子集可能并不是 Envoy 实例 EDS 端点的子集。 参考 Health Discovery Service (HDS) - github.com 点击关注【ServiceMesher】 微信公众号回复【加群】加入社区Copyright © servicemesher.com 2018-2019 all right reserved，powered by Gitbook Updated at 2019-03-10 16:35:08 "},"data-plane/envoy-advance-api.html":{"url":"data-plane/envoy-advance-api.html","title":"Envoy 高级 API","summary":"本文是介绍 Envoy 高级 API 的开篇。","keywords":"","body":"Envoy 高级 API 除了 xDS API，Envoy 还提供如下高级 API： MS（Metric Service）：将 metric 推送到远端服务器 RLS（Rate Limit Service）：速率限制服务 点击关注【ServiceMesher】 微信公众号回复【加群】加入社区Copyright © servicemesher.com 2018-2019 all right reserved，powered by Gitbook Updated at 2019-03-10 16:35:27 "},"data-plane/envoy-ms.html":{"url":"data-plane/envoy-ms.html","title":"MS（Metric 服务）","summary":"本文是介绍 Envoy 高级 API 的中的 MS。","keywords":"","body":"MS（Metric 服务） 点击关注【ServiceMesher】 微信公众号回复【加群】加入社区Copyright © servicemesher.com 2018-2019 all right reserved，powered by Gitbook Updated at 2019-03-10 16:35:37 "},"data-plane/envoy-rls.html":{"url":"data-plane/envoy-rls.html","title":"RLS（速率限制服务）","summary":"本文是介绍 Envoy 高级 API 的中的 RLS。","keywords":"","body":"RLS（速率限制服务） 点击关注【ServiceMesher】 微信公众号回复【加群】加入社区Copyright © servicemesher.com 2018-2019 all right reserved，powered by Gitbook Updated at 2019-03-10 16:35:45 "},"traffic-management/":{"url":"traffic-management/","title":"Istio 中的流量管理","summary":"本文是流量管理的开篇","keywords":"","body":"流量管理 这一章节将带大家了解 Istio 流量管理中的各种概念的含义及表示方法。 流量管理是 Isito 中的最基础功能，使用 Istio 的流量管理模型，本质上是将流量与基础设施扩容解耦，让运维人员可以通过 Pilot 指定流量遵循什么规则，而不是指定哪些 pod/VM 应该接收流量——Pilot 和智能 Envoy 代理会帮我们搞定。 所谓流量管理是指： 控制服务之间的路由：通过在 VirtualService 中的规则条件匹配来设置路由，可以在服务间拆分流量。 控制路由上流量的行为：设定好路由之后，就可以在路由上指定超时和重试机制，例如超时时间、重试次数等；做错误注入、设置断路器等。可以由 VirtualService 和 DestinationRule 共同完成。 显式地向网格中注册服务：显示地引入 Service Mesh 内部或外部的服务，纳入服务网格管理。由 ServiceEntry 实现。 控制网格边缘的南北向流量：为了管理进入 Istio service mesh 的南北向入口流量，需要创建 Gateway 对象并与 VirtualService 绑定。 关于流量管理的详细介绍请参考 Istio 官方文档。 参考 流量管理 - istio.io 点击关注【ServiceMesher】 微信公众号回复【加群】加入社区Copyright © servicemesher.com 2018-2019 all right reserved，powered by Gitbook Updated at 2019-03-13 17:48:25 "},"traffic-management/traffic-management-basic.html":{"url":"traffic-management/traffic-management-basic.html","title":"流量管理基础概念","summary":"本文介绍了 istio 中流量管理的基本概念。","keywords":"","body":"流量管理基础概念 下面将带您了解 Istio 流量管理相关的基础概念与配置示例。 VirtualService 在 Istio 服务网格中定义路由规则，控制流量路由到服务上的各种行为。 DestinationRule 是 VirtualService 路由生效后，配置应用与请求的策略集。 ServiceEntry 通常用于在 Istio 服务网格之外启用的服务请求。 Gateway 为 HTTP/TCP 流量配置负载均衡器，最常见的是在网格边缘的操作，以启用应用程序的入口流量。 EnvoyFilter 描述了针对代理服务的过滤器，用来定制由 Istio Pilot 生成的代理配置。一定要谨慎使用此功能。错误的配置内容一旦完成传播，可能会令整个服务网格陷入瘫痪状态。这一配置是用于对 Istio 网络系统内部实现进行变更的。 注：本文中的示例引用自 Istio 官方 Bookinfo 示例，见：Istio 代码库，且对于配置的讲解都以在 Kubernetes 中部署的服务为准。 VirtualService VirtualService 故名思义，就是虚拟服务，在 Istio 1.0 以前叫做 RouteRule。VirtualService 中定义了一系列针对指定服务的流量路由规则。每个路由规则都是针对特定协议的匹配规则。如果流量符合这些特征，就会根据规则发送到服务注册表中的目标服务（或者目标服务的子集或版本）。VirtualService 的详细定义和配置请参考通信路由。 注意：VirtualService 中的规则是按照在 YAML 文件中的顺序执行的，这就是为什么在存在多条规则时，需要慎重考虑优先级的原因。 配置说明 下面是 VirtualService 的配置说明。 字段 类型 描述 hosts string[] 必要字段：流量的目标主机。可以是带有通配符前缀的 DNS 名称，也可以是 IP 地址。根据所在平台情况，还可能使用短名称来代替 FQDN。这种场景下，短名称到 FQDN 的具体转换过程是要靠下层平台完成的。一个主机名只能在一个 VirtualService 中定义。同一个 VirtualService 中可以用于控制多个 HTTP 和 TCP 端口的流量属性。Kubernetes 用户注意：当使用服务的短名称时（例如使用 reviews，而不是 reviews.default.svc.cluster.local），Istio 会根据规则所在的命名空间来处理这一名称，而非服务所在的命名空间。假设 “default” 命名空间的一条规则中包含了一个 reviews 的 host 引用，就会被视为 reviews.default.svc.cluster.local，而不会考虑 reviews 服务所在的命名空间。为了避免可能的错误配置，建议使用 FQDN 来进行服务引用。 hosts 字段对 HTTP 和 TCP 服务都是有效的。网格中的服务也就是在服务注册表中注册的服务，必须使用他们的注册名进行引用；只有 Gateway 定义的服务才可以使用 IP 地址。 gateways string[] Gateway 名称列表，Sidecar 会据此使用路由。VirtualService 对象可以用于网格中的 Sidecar，也可以用于一个或多个 Gateway。这里公开的选择条件可以在协议相关的路由过滤条件中进行覆盖。保留字 mesh 用来指代网格中的所有 Sidecar。当这一字段被省略时，就会使用缺省值（mesh），也就是针对网格中的所有 Sidecar 生效。如果提供了 gateways 字段，这一规则就只会应用到声明的 Gateway 之中。要让规则同时对 Gateway 和网格内服务生效，需要显式的将 mesh 加入 gateways 列表。 http HTTPRoute[] HTTP 流量规则的有序列表。这个列表对名称前缀为 http-、http2-、grpc- 的服务端口，或者协议为 HTTP、HTTP2、GRPC 以及终结的 TLS，另外还有使用 HTTP、HTTP2 以及 GRPC 协议的 ServiceEntry 都是有效的。进入流量会使用匹配到的第一条规则。 tls TLSRoute[] 一个有序列表，对应的是透传 TLS 和 HTTPS 流量。路由过程通常利用 ClientHello 消息中的 SNI 来完成。TLS 路由通常应用在 https-、tls- 前缀的平台服务端口，或者经 Gateway 透传的 HTTPS、TLS 协议端口，以及使用 HTTPS 或者 TLS 协议的 ServiceEntry 端口上。注意：没有关联 VirtualService 的 https- 或者 tls- 端口流量会被视为透传 TCP 流量。 tcp TCPRoute[] 一个针对透传 TCP 流量的有序路由列表。TCP 路由对所有 HTTP 和 TLS 之外的端口生效。进入流量会使用匹配到的第一条规则。 示例 下面的例子中配置了一个名为 reviews 的 VirtualService，该配置的作用是将所有发送给 reviews 服务的流量发送到 v1 版本的子集。 apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: reviews spec: hosts: - reviews http: - route: - destination: host: reviews subset: v1 该配置中流量的目标主机是 reviews，如果该服务和规则部署在 Kubernetes 的 default namespace 下的话，对应于 Kubernetes 中的服务的 DNS 名称就是 reviews.default.svc.cluster.local。 我们在 hosts 配置了服务的名字只是表示该配置是针对 reviews.default.svc.cluster.local 的服务的路由规则，但是具体将对该服务的访问的流量路由到哪些服务的哪些实例上，就是要通过 destination 的配置了。 我们看到上面的 VirtualService 的 HTTP 路由中还定义了一个 destination。destination 用于定义在网络中可寻址的服务，请求或连接在经过路由规则的处理之后，就会被发送给 destination。destination.host 应该明确指向服务注册表中的一个服务。Istio 的服务注册表除包含平台服务注册表中的所有服务（例如 Kubernetes 服务、Consul 服务）之外，还包含了 ServiceEntry 资源所定义的服务。VirtualService 中只定义流量发送给哪个服务的路由规则，但是并不知道要发送的服务的地址是什么，这就需要 DestinationRule 来定义了。 subset 配置流量目的地的子集，下文会讲到。VirtualService 中其实可以除了 hosts 字段外其他什么都不配置，路由规则可以在 DestinationRule 中单独配置来覆盖此处的默认规则。 Subset subset 不属于 Istio 创建的 CRD，但是它是一条重要的配置信息，有必要单独说明下。subset 是服务端点的集合，可以用于 A/B 测试或者分版本路由等场景。参考 VirtualService 文档，其中会有更多这方面应用的例子。另外在 subset 中可以覆盖服务级别的即 VirtualService 中的定义的流量策略。 以下是subset 的配置信息。对于 Kubernetes 中的服务，一个 subset 相当于使用 label 的匹配条件选出来的 service。 字段 类型 描述 name string 必要字段。服务名和 subset 名称可以用于路由规则中的流量拆分。 labels map 必要字段。使用标签对服务注册表中的服务端点进行筛选。 trafficPolicy TrafficPolicy 应用到这一 subset 的流量策略。缺省情况下 subset 会继承 DestinationRule 级别的策略，这一字段的定义则会覆盖缺省的继承策略。 DestinationRule DestinationRule 所定义的策略，决定了经过路由处理之后的流量的访问策略。这些策略中可以定义负载均衡配置、连接池大小以及外部检测（用于在负载均衡池中对不健康主机进行识别和驱逐）配置。 配置说明 下面是 DestinationRule 的配置说明。 字段 类型 描述 name string 必要字段。服务名和 subset 名称可以用于路由规则中的流量拆分。 labels map 必要字段。使用标签对服务注册表中的服务端点进行筛选。 trafficPolicy TrafficPolicy 应用到这一子集的流量策略。缺省情况下子集会继承 DestinationRule 级别的策略，这一字段的定义则会覆盖缺省的继承策略。 示例 下面是一条对 productpage 服务的流量目的地策略的配置。 apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: reviews spec: host: reviews subsets: - name: v1 labels: version: v1 该路由策略将所有对 reviews 服务的流量路由到 v1 的 subset。 ServiceEntry Istio 服务网格内部会维护一个与平台无关的使用通用模型表示的服务注册表，当你的服务网格需要访问外部服务的时候，就需要使用 ServiceEntry 来添加服务注册。 EnvoyFilter EnvoyFilter 描述了针对代理服务的过滤器，用来定制由 Istio Pilot 生成的代理配置。一定要谨慎使用此功能。错误的配置内容一旦完成传播，可能会令整个服务网格陷入瘫痪状态。这一配置是用于对 Istio 网络系统内部实现进行变更的，属于高级配置，用于扩展 Envoy 中的过滤器的。 Gateway Gateway 为 HTTP/TCP 流量配置了一个负载均衡，多数情况下在网格边缘进行操作，用于启用一个服务的入口（ingress）流量，相当于前端代理。与 Kubernetes 的 Ingress 不同，Istio Gateway 只配置四层到六层的功能（例如开放端口或者 TLS 配置），而 Kubernetes 的 Ingress 是七层的。将 VirtualService 绑定到 Gateway 上，用户就可以使用标准的 Istio 规则来控制进入的 HTTP 和 TCP 流量。 Gateway 设置了一个集群外部流量访问集群中的某些服务的入口，而这些流量究竟如何路由到那些服务上则需要通过配置 VirtualServcie 来绑定。下面仍然以 productpage 这个服务来说明。 apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: bookinfo-gateway spec: selector: istio: ingressgateway # 使用默认的控制器 servers: - port: number: 80 name: http protocol: HTTP hosts: - \"*\" --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: bookinfo spec: hosts: - \"*\" gateways: - bookinfo-gateway http: - match: - uri: exact: /productpage - uri: exact: /login - uri: exact: /logout - uri: prefix: /api/v1/products route: - destination: host: productpage port: number: 9080 上面的例子中 bookinfo 这个 VirtualService 中绑定到了 bookinfo-gateway。bookinfo-gateway 使用了标签选择器选择对应的 Kubernetes pod，即下图中的 pod。 图片 - istio ingress gateway pod 我们再看下 istio-ingressgateway 的 YAML 安装配置。 # Deployment 配置 apiVersion: extensions/v1beta1 kind: Deployment metadata: name: istio-ingressgateway namespace: istio-system labels: app: ingressgateway chart: gateways-1.0.0 release: RELEASE-NAME heritage: Tiller app: istio-ingressgateway istio: ingressgateway spec: replicas: 1 template: metadata: labels: app: istio-ingressgateway istio: ingressgateway annotations: sidecar.istio.io/inject: \"false\" scheduler.alpha.kubernetes.io/critical-pod: \"\" spec: serviceAccountName: istio-ingressgateway-service-account containers: - name: ingressgateway image: \"gcr.io/istio-release/proxyv2:1.0.0\" # 容器启动命令入口是 /usr/local/bin/pilot-agent，后面跟参数 proxy 就会启动一个 Envoy 进程 imagePullPolicy: IfNotPresent ports: - containerPort: 80 - containerPort: 443 - containerPort: 31400 - containerPort: 15011 - containerPort: 8060 - containerPort: 15030 - containerPort: 15031 args: - proxy - router - -v - \"2\" - --discoveryRefreshDelay - '1s' #discoveryRefreshDelay - --drainDuration - '45s' #drainDuration - --parentShutdownDuration - '1m0s' #parentShutdownDuration - --connectTimeout - '10s' #connectTimeout - --serviceCluster - istio-ingressgateway - --zipkinAddress - zipkin:9411 - --statsdUdpAddress - istio-statsd-prom-bridge:9125 - --proxyAdminPort - \"15000\" - --controlPlaneAuthPolicy - NONE - --discoveryAddress - istio-pilot.istio-system:8080 resources: requests: cpu: 10m env: - name: POD_NAME valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.namespace - name: INSTANCE_IP valueFrom: fieldRef: apiVersion: v1 fieldPath: status.podIP - name: ISTIO_META_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name volumeMounts: ... # 服务配置 --- apiVersion: v1 kind: Service metadata: name: istio-ingressgateway namespace: istio-system annotations: labels: chart: gateways-1.0.0 release: RELEASE-NAME heritage: Tiller app: istio-ingressgateway istio: ingressgateway spec: type: NodePort selector: app: istio-ingressgateway istio: ingressgateway ports: - name: http2 # 将 ingressgateway 的 80 端口映射到节点的 31380 端口以代理 HTTP 请求 nodePort: 31380 port: 80 targetPort: 80 - name: https nodePort: 31390 port: 443 - name: tcp nodePort: 31400 port: 31400 - name: tcp-pilot-grpc-tls port: 15011 targetPort: 15011 - name: tcp-citadel-grpc-tls port: 8060 targetPort: 8060 - name: http2-prometheus port: 15030 targetPort: 15030 - name: http2-grafana port: 15031 targetPort: 15031 我们看到 ingressgateway 使用的是 proxyv2 镜像，该镜像容器的启动命令入口是 /usr/local/bin/pilot-agent，后面跟参数 proxy 就会启动一个 Envoy 进程，因此 Envoy 既作为 sidecar 也作为边缘代理，egressgateway 的情况也是类似，只不过它控制的是集群内部对外集群外部的请求。这正好验证了本文开头中所画的 Istio Pilot 架构图。请求 /productpage 、/login、/logout、/api/v1/products 这些 URL 的流量转发给 productpage 服务的 9080 端口，而这些流量进入集群内又是经过 ingressgateway pod 代理的，通过访问 ingressgateway pod 所在的宿主机的 31380 端口进入集群内部的。 示例 我们以官方的 bookinfo 示例来解析流量管理配置。下图是 VirtualService 和 DestinationRule 的示意图，其中只显示了 productpage 和 reviews 服务。 图片 - VirtualSerivce 和 DestimationRule 示意图 在前提条件中我部署了该示例，并列出了该示例中的所有 pod，现在我们使用 istioctl 命令来启动查看 productpage-v1-745ffc55b7-2l2lw pod 中的流量配置。 查看 pod 中 Envoy sidecar 的启动配置信息 Bootstrap 消息是 Envoy 配置的根本来源，Bootstrap 消息的一个关键的概念是静态和动态资源的之间的区别。例如 Listener 或 Cluster 这些资源既可以从 static_resources 静态的获得也可以从 dynamic_resources 中配置的 LDS 或 CDS 之类的 xDS 服务获取。关于 xDS 服务的详解请参考 Envoy 中的 xDS REST 和 gRPC 协议详解。 $ istioctl proxy-config bootstrap productpage-v1-745ffc55b7-2l2lw -o json { \"bootstrap\": { \"node\": { \"id\": \"sidecar~172.33.78.10~productpage-v1-745ffc55b7-2l2lw.default~default.svc.cluster.local\", \"cluster\": \"productpage\", \"metadata\": { \"INTERCEPTION_MODE\": \"REDIRECT\", \"ISTIO_PROXY_SHA\": \"istio-proxy:6166ae7ebac7f630206b2fe4e6767516bf198313\", \"ISTIO_PROXY_VERSION\": \"1.0.0\", \"ISTIO_VERSION\": \"1.0.0\", \"POD_NAME\": \"productpage-v1-745ffc55b7-2l2lw\", \"istio\": \"sidecar\" }, \"buildVersion\": \"0/1.8.0-dev//RELEASE\" }, \"staticResources\": { # Envoy 的静态配置，除非销毁后重设，否则不会改变，配置中会明确指定每个上游主机的已解析网络名称（ IP 地址、端口、unix 域套接字等）。 \"clusters\": [ { \"name\": \"xds-grpc\", \"type\": \"STRICT_DNS\", \"connectTimeout\": \"10.000s\", \"hosts\": [ { # istio-pilot 的地址，指定控制平面地址，这个必须是通过静态的方式配置的 \"socketAddress\": { \"address\": \"istio-pilot.istio-system\", \"portValue\": 15010 } } ], \"circuitBreakers\": { # 断路器配置 \"thresholds\": [ { \"maxConnections\": 100000, \"maxPendingRequests\": 100000, \"maxRequests\": 100000 }, { \"priority\": \"HIGH\", \"maxConnections\": 100000, \"maxPendingRequests\": 100000, \"maxRequests\": 100000 } ] }, \"http2ProtocolOptions\": { }, \"upstreamConnectionOptions\": { # 上游连接选项 \"tcpKeepalive\": { \"keepaliveTime\": 300 } } }, { # zipkin 分布式追踪地址配置 \"name\": \"zipkin\", \"type\": \"STRICT_DNS\", \"connectTimeout\": \"1.000s\", \"hosts\": [ { \"socketAddress\": { \"address\": \"zipkin.istio-system\", \"portValue\": 9411 } } ] } ] }, # 以下是动态配置 \"dynamicResources\": { \"ldsConfig\": { # Listener Discovery Service 配置，直接使用 ADS 配置，此处不用配置 \"ads\": { } }, \"cdsConfig\": { # Cluster Discovery Service 配置，直接使用 ADS 配置，此处不用配置 \"ads\": { } }, \"adsConfig\": { # Aggregated Discovery Service 配置，ADS 中集成了 LDS、RDS、CDS \"apiType\": \"GRPC\", \"grpcServices\": [ { \"envoyGrpc\": { \"clusterName\": \"xds-grpc\" } } ], \"refreshDelay\": \"1.000s\" } }, \"statsSinks\": [ # metric 汇聚的地址 { \"name\": \"envoy.statsd\", \"config\": { \"address\": { \"socket_address\": { \"address\": \"10.254.109.175\", \"port_value\": 9125 } } } } ], \"statsConfig\": { \"useAllDefaultTags\": false }, \"tracing\": { # zipkin 地址 \"http\": { \"name\": \"envoy.zipkin\", \"config\": { \"collector_cluster\": \"zipkin\" } } }, \"admin\": { \"accessLogPath\": \"/dev/stdout\", \"address\": { \"socketAddress\": { \"address\": \"127.0.0.1\", \"portValue\": 15000 } } } }, \"lastUpdated\": \"2018-09-04T03:38:45.645Z\" } 以上为初始信息。 创建一个名为 reviews 的 VirtualService。 $ cat 上面的 VirtualService 定义只是定义了访问 reviews 服务的流量要全部流向 reviews服务的 v1子集，至于哪些实例是 v1 子集，VirtualService 中并没有定义，这就需要再创建个 DestinationRule。 $ cat 同时还可以为每个 subset 设置负载均衡规则。这里面也可以同时创建多个子集，例如同时创建3个 subset 分别对应3个版本的实例。 $ cat 同时配置了三个 subset 当你需要切分流量时可以直接修改 VirtualService 中 destination 里的 subset 即可，还可以根据百分比拆分流量，配置超时和重试，进行错误注入等，详见流量管理 当然上面这个例子中只是简单的将流量全部导到某个 VirtualService 的 subset 中，还可以根据其他限定条件如 HTTP headers、pod 的 label、URL 等。 此时再查询 productpage-v1-745ffc55b7-2l2lw pod 的配置信息。 $ istioctl proxy-config clusters productpage-v1-8d69b45c-bcjqv|grep reviews reviews.default.svc.cluster.local 9080 - outbound EDS reviews.default.svc.cluster.local 9080 v1 outbound EDS reviews.default.svc.cluster.local 9080 v2 outbound EDS reviews.default.svc.cluster.local 9080 v3 outbound EDS 可以看到 reviews 服务的 EDS 设置中包含了3个 subset，另外读者还可以自己运行 istioctl proxy-config listeners 和 istioctl proxy-config route 来查询 pod 的监听器和路由配置。 参考 流量管理 - istio.io 通信路由 - istio.io istioctl 指南 - istio.io Envoy 官方文档中文版 - servicemesher.com Envoy v2 API 概览 - servicemesher.com Envoy 中的 xDS REST 和 gRPC 协议详解 - servicemesher.com 点击关注【ServiceMesher】 微信公众号回复【加群】加入社区Copyright © servicemesher.com 2018-2019 all right reserved，powered by Gitbook Updated at 2019-03-10 16:36:05 "},"traffic-management/sidecar-traffic-routing-deep-dive.html":{"url":"traffic-management/sidecar-traffic-routing-deep-dive.html","title":"Istio 中的 Sidecar 的流量路由详解","summary":"本文以 Istio 官方的 bookinfo 示例来讲解在进入 Pod 的流量被 iptables 转交给 Envoy sidecar 后，Envoy 是如何做路由转发的，详述了 Inbound 和 Outbound 处理过程。","keywords":"","body":"Istio 中的 Sidecar 的流量路由详解 本文以 Istio 官方的 bookinfo 示例来讲解在进入 Pod 的流量被 iptables 转交给 Envoy sidecar 后，Envoy 是如何做路由转发的，详述了 Inbound 和 Outbound 处理过程。关于流量拦截的详细分析请参考理解 Istio Service Mesh 中 Envoy 代理 Sidecar 注入及流量劫持。 下面是 Istio 官方提供的 bookinfo 的请求流程图，假设 bookinfo 应用的所有服务中没有配置 DestinationRule。 图片 - Bookinfo 示例 下面是 Istio 自身组件与 Bookinfo 示例的连接关系图，我们可以看到所有的 HTTP 连接都在 9080 端口监听。 图片 - Bookinfo 示例与 Istio 组件连接关系图 可以在 Google Drive 上下载原图。 Sidecar 注入及流量劫持步骤概述 下面是从 Sidecar 注入、Pod 启动到 Sidecar proxy 拦截流量及 Envoy 处理路由的步骤概览。 1. Kubernetes 通过 Admission Controller 自动注入，或者用户使用 istioctl 命令手动注入 sidecar 容器。 2. 应用 YAML 配置部署应用，此时 Kubernetes API server 接收到的服务创建配置文件中已经包含了 Init 容器及 sidecar proxy。 3. 在 sidecar proxy 容器和应用容器启动之前，首先运行 Init 容器，Init 容器用于设置 iptables（Istio 中默认的流量拦截方式，还可以使用 BPF、IPVS 等方式） 将进入 pod 的流量劫持到 Envoy sidecar proxy。所有 TCP 流量（Envoy 目前只支持 TCP 流量）将被 sidecar 劫持，其他协议的流量将按原来的目的地请求。 4. 启动 Pod 中的 Envoy sidecar proxy 和应用程序容器。这一步的过程请参考通过管理接口获取完整配置。 Sidecar proxy 与应用容器的启动顺序问题 启动 sidecar proxy 和应用容器，究竟哪个容器先启动呢？正常情况是 Envoy Sidecar 和应用程序容器全部启动完成后再开始接收流量请求。但是我们无法预料哪个容器会先启动，那么容器启动顺序是否会对 Envoy 劫持流量有影响呢？答案是肯定的，不过分为以下两种情况。 情况1：应用容器先启动，而 sidecar proxy 仍未就绪 这种情况下，流量被 iptables 转移到 15001 端口，而 Pod 中没有监听该端口，TCP 链接就无法建立，请求失败。 情况2：Sidecar 先启动，请求到达而应用程序仍未就绪 这种情况下请求也肯定会失败，至于是在哪一步开始失败的，留给读者来思考。 问题：如果为 sidecar proxy 和应用程序容器添加就绪和存活探针是否可以解决该问题呢？ 5. 不论是进入还是从 Pod 发出的 TCP 请求都会被 iptables 劫持，inbound 流量被劫持后经 Inbound Handler 处理后转交给应用程序容器处理，outbound 流量被 iptables 劫持后转交给 Outbound Handler 处理，并确定转发的 upstream 和 Endpoint。 6. Sidecar proxy 请求 Pilot 使用 xDS 协议同步 Envoy 配置，其中包括 LDS、EDS、CDS 等，不过为了保证更新的顺序，Envoy 会直接使用 ADS 向 Pilot 请求配置更新。 Envoy 如何处理路由转发 下图展示的是 productpage 服务请求访问 http://reviews.default.svc.cluster.local:9080/，当流量进入 reviews 服务内部时，reviews 服务内部的 Envoy Sidecar 是如何做流量拦截和路由转发的。可以在 Google Drive 上下载原图。 图片 - Envoy sidecar 流量劫持与路由转发示意图 第一步开始时，productpage Pod 中的 Envoy sidecar 已经通过 EDS 选择出了要请求的 reviews 服务的一个 Pod，知晓了其 IP 地址，发送 TCP 连接请求。 Istio 官网中的 Envoy 配置深度解析中是以发起 HTTP 请求的一方来详述 Envoy 做流量转发的过程，而本文中考虑的是接受 downstream 的流量的一方，它既要接收 downstream 发来的请求，自己还需要请求其他服务，例如 reviews 服务中的 Pod 还需要请求 ratings 服务。 reviews 服务有三个版本，每个版本有一个实例，三个版本中的 sidecar 工作步骤类似，下文只以 reviews-v1-cb8655c75-b97zc 这一个 Pod 中的 Sidecar 流量转发步骤来说明。 理解 Inbound Handler Inbound handler 的作用是将 iptables 拦截到的 downstream 的流量转交给 localhost，与 Pod 内的应用程序容器建立连接。 查看下 reviews-v1-cb8655c75-b97zc pod 中的 Listener。 运行 istioctl pc listener reviews-v1-cb8655c75-b97zc 查看该 Pod 中的具有哪些 Listener。 ADDRESS PORT TYPE 172.33.3.3 9080 HTTP 当来自 productpage 的流量抵达 reviews Pod 的时候已经，downstream 必须明确知道 Pod 的 IP 地址为 172.33.3.3 所以才会访问该 Pod，所以该请求是 172.33.3.3:9080。 virtual Listener 从该 Pod 的 Listener 列表中可以看到，0.0.0.0:15001/TCP 的 Listener（其实际名字是 virtual）监听所有的 Inbound 流量，下面是该 Listener 的详细配置。 { \"name\": \"virtual\", \"address\": { \"socketAddress\": { \"address\": \"0.0.0.0\", \"portValue\": 15001 } }, \"filterChains\": [ { \"filters\": [ { \"name\": \"envoy.tcp_proxy\", \"config\": { \"cluster\": \"BlackHoleCluster\", \"stat_prefix\": \"BlackHoleCluster\" } } ] } ], \"useOriginalDst\": true } UseOriginalDst：从配置中可以看出 useOriginalDst 配置指定为 true，这是一个布尔值，缺省为 false，使用 iptables 重定向连接时，proxy 接收的端口可能与原始目的地址的端口不一样，如此处 proxy 接收的端口为 15001，而原始目的地端口为 9080。当此标志设置为 true 时，Listener 将连接重定向到与原始目的地址关联的 Listener，此处为 172.33.3.3:9080。如果没有与原始目的地址关联的 Listener，则连接由接收它的 Listener 处理，即该 virtual Listener，经过 envoy.tcp_proxy 过滤器处理转发给 BlackHoleCluster，这个 Cluster 的作用正如它的名字，当 Envoy 找不到匹配的虚拟监听器时，就会将请求发送给它，并返回 404。这个将于下文提到的 Listener 中设置 bindToPort 相呼应。 注意：该参数将被废弃，请使用原始目的地址的 Listener filter 替代。该参数的主要用途是：Envoy 通过监听 15001 端口将 iptables 拦截的流量经由其他 Listener 处理而不是直接转发出去，详情见 Virtual Listener。 Listener 172.33.3.3_9080 上文说到进入 Inbound handler 的流量被 virtual Listener 转移到 172.33.3.3_9080 Listener，我们在查看下该 Listener 配置。 运行 istioctl pc listener reviews-v1-cb8655c75-b97zc --address 172.33.3.3 --port 9080 -o json 查看。 [{ \"name\": \"172.33.3.3_9080\", \"address\": { \"socketAddress\": { \"address\": \"172.33.3.3\", \"portValue\": 9080 } }, \"filterChains\": [ { \"filterChainMatch\": { \"transportProtocol\": \"raw_buffer\" }, \"filters\": [ { \"name\": \"envoy.http_connection_manager\", \"config\": { ... \"route_config\": { \"name\": \"inbound|9080||reviews.default.svc.cluster.local\", \"validate_clusters\": false, \"virtual_hosts\": [ { \"domains\": [ \"*\" ], \"name\": \"inbound|http|9080\", \"routes\": [ { ... \"route\": { \"cluster\": \"inbound|9080||reviews.default.svc.cluster.local\", \"max_grpc_timeout\": \"0.000s\", \"timeout\": \"0.000s\" } } ] } ] }, \"use_remote_address\": false, ... } } ]， \"deprecatedV1\": { \"bindToPort\": false } ... }, { \"filterChainMatch\": { \"transportProtocol\": \"tls\" }, \"tlsContext\": {... }, \"filters\": [... ] } ], ... }] bindToPort：注意其中有一个 bindToPort 的配置，其值为 false，该配置的缺省值为 true，表示将 Listener 绑定到端口上，此处设置为 false 则该 Listener 只能处理其他 Listener 转移过来的流量，即上文所说的 virtual Listener，我们看其中的 filterChains.filters 中的 envoy.http_connection_manager 配置部分： \"route_config\": { \"name\": \"inbound|9080||reviews.default.svc.cluster.local\", \"validate_clusters\": false, \"virtual_hosts\": [ { \"domains\": [ \"*\" ], \"name\": \"inbound|http|9080\", \"routes\": [ { ... \"route\": { \"cluster\": \"inbound|9080||reviews.default.svc.cluster.local\", \"max_grpc_timeout\": \"0.000s\", \"timeout\": \"0.000s\" } } ] } ] } 该配置表示流量将转交给 Cluster inbound|9080||reviews.default.svc.cluster.local 处理。 Cluster inbound|9080||reviews.default.svc.cluster.local 运行 istioctl pc cluster reviews-v1-cb8655c75-b97zc --fqdn reviews.default.svc.cluster.local --direction inbound -o json 查看该 Cluster 的配置如下。 [ { \"name\": \"inbound|9080||reviews.default.svc.cluster.local\", \"connectTimeout\": \"1.000s\", \"hosts\": [ { \"socketAddress\": { \"address\": \"127.0.0.1\", \"portValue\": 9080 } } ], \"circuitBreakers\": { \"thresholds\": [ {} ] } } ] 可以看到该 Cluster 的 Endpoint 直接对应的就是 localhost，再经过 iptables 转发流量就被应用程序容器消费了。 理解 Outbound Handler 因为 reviews 会向 ratings 服务发送 HTTP 请求，请求的地址是：http://ratings.default.svc.cluster.local:9080/，Outbound handler 的作用是将 iptables 拦截到的本地应用程序发出的流量，经由 Envoy 判断如何路由到 upstream。 应用程序容器发出的请求为 Outbound 流量，被 iptables 劫持后转移给 Envoy Outbound handler 处理，然后经过 virtual Listener、0.0.0.0_9080 Listener，然后通过 Route 9080 找到 upstream 的 cluster，进而通过 EDS 找到 Endpoint 执行路由动作。这一部分可以参考 Istio 官网中的 Envoy 深度配置解析。 Route 9080 reviews 会请求 ratings 服务，运行 istioctl proxy-config routes reviews-v1-cb8655c75-b97zc --name 9080 -o json 查看 route 配置，因为 Envoy 会根据 HTTP header 中的 domains 来匹配 VirtualHost，所以下面只列举了 ratings.default.svc.cluster.local:9080 这一个 VirtualHost。 [{ \"name\": \"ratings.default.svc.cluster.local:9080\", \"domains\": [ \"ratings.default.svc.cluster.local\", \"ratings.default.svc.cluster.local:9080\", \"ratings\", \"ratings:9080\", \"ratings.default.svc.cluster\", \"ratings.default.svc.cluster:9080\", \"ratings.default.svc\", \"ratings.default.svc:9080\", \"ratings.default\", \"ratings.default:9080\", \"10.254.234.130\", \"10.254.234.130:9080\" ], \"routes\": [ { \"match\": { \"prefix\": \"/\" }, \"route\": { \"cluster\": \"outbound|9080||ratings.default.svc.cluster.local\", \"timeout\": \"0.000s\", \"maxGrpcTimeout\": \"0.000s\" }, \"decorator\": { \"operation\": \"ratings.default.svc.cluster.local:9080/*\" }, \"perFilterConfig\": {... } } ] }, ..] 从该 Virtual Host 配置中可以看到将流量路由到 Cluster outbound|9080||ratings.default.svc.cluster.local。 Endpoint outbound|9080||ratings.default.svc.cluster.local Istio 1.1 以前版本不支持使用 istioctl 命令直接查询 Cluster 的 Endpoint，可以使用查询 Pilot 的 debug 端点的方式折中。 kubectl exec reviews-v1-cb8655c75-b97zc -c istio-proxy curl http://istio-pilot.istio-system.svc.cluster.local:9093/debug/edsz > endpoints.json endpoints.json 文件中包含了所有 Cluster 的 Endpoint 信息，我们只选取其中的 outbound|9080||ratings.default.svc.cluster.local Cluster 的结果如下。 { \"clusterName\": \"outbound|9080||ratings.default.svc.cluster.local\", \"endpoints\": [ { \"locality\": { }, \"lbEndpoints\": [ { \"endpoint\": { \"address\": { \"socketAddress\": { \"address\": \"172.33.100.2\", \"portValue\": 9080 } } }, \"metadata\": { \"filterMetadata\": { \"istio\": { \"uid\": \"kubernetes://ratings-v1-8558d4458d-ns6lk.default\" } } } } ] } ] } Endpoint 可以是一个或多个，Envoy 将根据一定规则选择适当的 Endpoint 来路由。 注：Istio 1.1 将支持 istioctl pc endpoint 命令来查询 Endpoint。 参考 调试 Envoy 和 Pilot - istio.io 理解 Istio Service Mesh 中 Envoy 代理 Sidecar 注入及流量劫持 - jimmysong.io Istio流量管理实现机制深度解析 - zhaohuabing.com 点击关注【ServiceMesher】 微信公众号回复【加群】加入社区Copyright © servicemesher.com 2018-2019 all right reserved，powered by Gitbook Updated at 2019-03-15 11:24:39 "},"best-practices/how-to-implement-ingress-gateway.html":{"url":"best-practices/how-to-implement-ingress-gateway.html","title":"为服务网格选择入口网关","summary":"本文将对Kubernetes和Istio对外提供服务的各种方式进行详细介绍和对比分析，并根据分析结果提出一个可用于产品部署的解决方案。","keywords":"","body":"为服务网格选择入口网关 在启用了Istio服务网格的Kubernetes集群中，缺省情况下只能在集群内部访问网格中的服务，要如何才能从外部网络访问这些服务呢？ Kubernetes和Istio提供了NodePort，LoadBalancer，Kubernetes Ingress，Istio Gateway等多种外部流量入口的方式，面对这么多种方式，我们在产品部署中应该如何选择？ 本文将对Kubernetes和Istio对外提供服务的各种方式进行详细介绍和对比分析，并根据分析结果提出一个可用于产品部署的解决方案。 说明：阅读本文要求读者了解Kubernetes和Istio的基本概念，包括Pod、Service、NodePort、LoadBalancer、Ingress、Gateway、VirtualService等。如对这些概念不熟悉，可以在阅读过程中参考文后的相关链接。 内部服务间的通信 首先，我们来回顾一下Kubernetes集群内部各个服务之间相互访问的方法。 Cluster IP Kubernetes以Pod作为应用部署的最小单位。kubernetes会根据Pod的声明对其进行调度，包括创建、销毁、迁移、水平伸缩等，因此Pod 的IP地址不是固定的，不方便直接采用Pod IP对服务进行访问。 为解决该问题，Kubernetes提供了Service资源，Service对提供同一个服务的多个Pod进行聚合。一个Service提供一个虚拟的Cluster IP，后端对应一个或者多个提供服务的Pod。在集群中访问该Service时，采用Cluster IP即可，Kube-proxy负责将发送到Cluster IP的请求转发到后端的Pod上。 Kube-proxy是一个运行在每个节点上的go应用程序，支持三种工作模式： userspace 模式 该模式下kube-proxy会为每一个Service创建一个监听端口。发向Cluster IP的请求被Iptables规则重定向到Kube-proxy监听的端口上，Kube-proxy根据LB算法选择一个提供服务的Pod并和其建立链接，以将请求转发到Pod上。 该模式下，Kube-proxy充当了一个四层Load balancer的角色。由于kube-proxy运行在userspace中，在进行转发处理时会增加两次内核和用户空间之间的数据拷贝，效率较另外两种模式低一些；好处是当后端的Pod不可用时，kube-proxy可以重试其他Pod。 图片 - Kube-proxy userspace模式 图片来自：Kubernetes官网文档 iptables 模式 为了避免增加内核和用户空间的数据拷贝操作，提高转发效率，Kube-proxy提供了iptables模式。在该模式下，Kube-proxy为service后端的每个Pod创建对应的iptables规则，直接将发向Cluster IP的请求重定向到一个Pod IP。 该模式下Kube-proxy不承担四层代理的角色，只负责创建iptables规则。该模式的优点是较userspace模式效率更高，但不能提供灵活的LB策略，当后端Pod不可用时也无法进行重试。 图片 - Kube-proxy iptables模式 图片来自：Kubernetes官网文档 ipvs 模式 该模式和iptables类似，kube-proxy监控Pod的变化并创建相应的ipvs rules。ipvs也是在kernel模式下通过netfilter实现的，但采用了hash table来存储规则，因此在规则较多的情况下，Ipvs相对iptables转发效率更高。除此以外，ipvs支持更多的LB算法。如果要设置kube-proxy为ipvs模式，必须在操作系统中安装IPVS内核模块。 图片 - Kube-proxy ipvs模式 图片来自：Kubernetes官网文档 Istio Sidecar Proxy Cluster IP解决了服务之间相互访问的问题，但从上面Kube-proxy的三种模式可以看到，Cluster IP的方式只提供了服务发现和基本的LB功能。如果要为服务间的通信应用灵活的路由规则以及提供Metrics collection，distributed tracing等服务管控功能,就必须得依靠Istio提供的服务网格能力了。 在Kubernetes中部署Istio后，Istio通过iptables和Sidecar Proxy接管服务之间的通信，服务间的相互通信不再通过Kube-proxy，而是通过Istio的Sidecar Proxy进行。请求流程是这样的：Client发起的请求被iptables重定向到Sidecar Proxy，Sidecar Proxy根据从控制面获取的服务发现信息和路由规则，选择一个后端的Server Pod创建链接，代理并转发Client的请求。 Istio Sidecar Proxy和Kube-proxy的userspace模式的工作机制类似，都是通过在用户空间的一个代理来实现客户端请求的转发和后端多个Pod之间的负载均衡。两者的不同点是：Kube-Proxy工作在四层，而Sidecar Proxy则是一个七层代理，可以针对HTTP，GRPS等应用层的语义进行处理和转发，因此功能更为强大，可以配合控制面实现更为灵活的路由规则和服务管控功能。 图片 - Istio Sidecar Proxy 如何从外部网络访问 Kubernetes的Pod IP和Cluster IP都只能在集群内部访问，而我们通常需要从外部网络上访问集群中的某些服务，Kubernetes提供了下述几种方式来为集群提供外部流量入口。 NodePort NodePort在集群中的主机节点上为Service提供一个代理端口，以允许从主机网络上对Service进行访问。Kubernetes官网文档只介绍了NodePort的功能，并未对其实现原理进行解释。下面我们通过实验来分析NodePort的实现机制。 www.katacoda.com 这个网站提供了一个交互式的Kubernetes playground，注册即可免费实验kubernetes的相关功能，下面我们就使用Katacoda来分析Nodeport的实现原理。 在浏览器中输入这个网址：https://www.katacoda.com/courses/kubernetes/networking-introduction， 打开后会提供了一个实验用的Kubernetes集群，并可以通过网元模拟Terminal连接到集群的Master节点。 执行下面的命令创建一个nodeport类型的service。 kubectl apply -f nodeport.yaml 查看创建的service，可以看到kubernetes创建了一个名为webapp-nodeport-svc的service，并为该service在主机节点上创建了30080这个Nodeport。 master $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 443/TCP 36m webapp1-nodeport-svc NodePort 10.103.188.73 80:30080/TCP 3m webapp-nodeport-svc后端对应两个Pod，其Pod的IP分别为10.32.0.3和10.32.0.5。 master $ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IPNODE NOMINATED NODE webapp1-nodeport-deployment-785989576b-cjc5b 1/1 Running 0 2m 10.32.0.3 webapp1-nodeport-deployment-785989576b-tpfqr 1/1 Running 0 2m 10.32.0.5 通过netstat命令可以看到Kube-proxy在主机网络上创建了30080监听端口，用于接收从主机网络进入的外部流量。 master $ netstat -lnp|grep 30080 tcp6 0 0 :::30080 :::* LISTEN 7427/kube-proxy 下面是Kube-proxy创建的相关iptables规则以及对应的说明。可以看到Kube-proxy为Nodeport创建了相应的IPtable规则，将发向30080这个主机端口上的流量重定向到了后端的两个Pod IP上。 iptables-save > iptables-dump # Generated by iptables-save v1.6.0 on Thu Mar 28 07:33:57 2019 *nat # Nodeport规则链 :KUBE-NODEPORTS - [0:0] # Service规则链 :KUBE-SERVICES - [0:0] # Nodeport和Service共用的规则链 :KUBE-SVC-J2DWGRZTH4C2LPA4 - [0:0] :KUBE-SEP-4CGFRVESQ3AECDE7 - [0:0] :KUBE-SEP-YLXG4RMKAICGY2B3 - [0:0] # 将host上30080端口的外部tcp流量转到KUBE-SVC-J2DWGRZTH4C2LPA4链 -A KUBE-NODEPORTS -p tcp -m comment --comment \"default/webapp1-nodeport-svc:\" -m tcp --dport 30080 -j KUBE-SVC-J2DWGRZTH4C2LPA4 #将发送到Cluster IP 10.103.188.73的内部流量转到KUBE-SVC-J2DWGRZTH4C2LPA4链 KUBE-SERVICES -d 10.103.188.73/32 -p tcp -m comment --comment \"default/webapp1-nodeport-svc: cluster IP\" -m tcp --dport 80 -j KUBE-SVC-J2DWGRZTH4C2LPA4 #将发送到webapp1-nodeport-svc的流量转交到第一个Pod（10.32.0.3）相关的规则链上，比例为50% -A KUBE-SVC-J2DWGRZTH4C2LPA4 -m comment --comment \"default/webapp1-nodeport-svc:\" -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-YLXG4RMKAICGY2B3 #将发送到webapp1-nodeport-svc的流量转交到第二个Pod（10.32.0.5）相关的规则链上 -A KUBE-SVC-J2DWGRZTH4C2LPA4 -m comment --comment \"default/webapp1-nodeport-svc:\" -j KUBE-SEP-4CGFRVESQ3AECDE7 #将请求重定向到Pod 10.32.0.3 -A KUBE-SEP-YLXG4RMKAICGY2B3 -p tcp -m comment --comment \"default/webapp1-nodeport-svc:\" -m tcp -j DNAT --to-destination 10.32.0.3:80 #将请求重定向到Pod 10.32.0.5 -A KUBE-SEP-4CGFRVESQ3AECDE7 -p tcp -m comment --comment \"default/webapp1-nodeport-svc:\" -m tcp -j DNAT --to-destination 10.32.0.5:80 从上面的实验可以看到，通过将一个Service定义为NodePort类型，Kubernetes会通过集群中node上的Kube-proxy为该Service在主机网络上创建一个监听端口。Kube-proxy并不会直接接收该主机端口进入的流量，而是会创建相应的Iptables规则，并通过Iptables将从该端口收到的流量直接转发到后端的Pod中。 NodePort的流量转发机制和Cluster IP的iptables模式类似，唯一不同之处是在主机网络上开了一个“NodePort”来接受外部流量。从上面的规则也可以看出，在创建Nodeport时，Kube-proxy也会同时为Service创建Cluster IP相关的iptables规则。 备注：除采用iptables进行流量转发，NodePort应该也可以提供userspace模式以及ipvs模式，这里未就这两种模式进行实验验证。 从分析得知，在NodePort模式下，集群内外部的通讯如下图所示： 图片 - NodePort LoadBalancer NodePort提供了一种从外部网络访问Kubernetes集群内部Service的方法，但该方法存在下面一些限制，导致这种方式主要适用于程序开发，不适合用于产品部署。 Kubernetes cluster host的IP必须是一个well-known IP，即客户端必须知道该IP。但Cluster中的host是被作为资源池看待的，可以增加删除，每个host的IP一般也是动态分配的，因此并不能认为host IP对客户端而言是well-known IP。 客户端访问某一个固定的host IP的方式存在单点故障。假如一台host宕机了，kubernetes cluster会把应用 reload到另一节点上，但客户端就无法通过该host的nodeport访问应用了。 通过一个主机节点作为网络入口，在网络流量较大时存在性能瓶颈。 为了解决这些问题，Kubernetes提供了LoadBalancer。通过将Service定义为LoadBalancer类型，Kubernetes在主机节点的NodePort前提供了一个四层的负载均衡器。该四层负载均衡器负责将外部网络流量分发到后面的多个节点的NodePort端口上。 下图展示了Kubernetes如何通过LoadBalancer方式对外提供流量入口，图中LoadBalancer后面接入了两个主机节点上的NodePort，后端部署了三个Pod提供服务。根据集群的规模，可以在LoadBalancer后面可以接入更多的主机节点，以进行负荷分担。 图片 - NodeBalancer 备注：LoadBalancer类型需要云服务提供商的支持，Service中的定义只是在Kubernetes配置文件中提出了一个要求，即为该Service创建Load Balancer，至于如何创建则是由Google Cloud或Amazon Cloud等云服务商提供的，创建的Load Balancer的过程不在Kubernetes Cluster的管理范围中。 目前WS, Azure, CloudStack, GCE 和 OpenStack 等主流的公有云和私有云提供商都可以为Kubernetes提供Load Balancer。一般来说，公有云提供商还会为Load Balancer提供一个External IP，以提供Internet接入。如果你的产品没有使用云提供商，而是自建Kubernetes Cluster，则需要自己提供LoadBalancer。 Ingress LoadBalancer类型的Service提供的是四层负载均衡器，当只需要向外暴露一个服务的时候，采用这种方式是没有问题的。但当一个应用需要对外提供多个服务时，采用该方式则要求为每一个四层服务（IP+Port）都创建一个外部load balancer。 一般来说，同一个应用的多个服务/资源会放在同一个域名下，在这种情况下，创建多个Load balancer是完全没有必要的，反而带来了额外的开销和管理成本。另外直接将服务暴露给外部用户也会导致了前端和后端的耦合，影响了后端架构的灵活性，如果以后由于业务需求对服务进行调整会直接影响到客户端。 在这种情况下，我们可以通过使用Kubernetes Ingress来统一网络入口。Kubernetes Ingress声明了一个应用层（OSI七层）的负载均衡器，可以根据HTTP请求的内容将来自同一个TCP端口的请求分发到不同的Kubernetes Service，其功能包括： 按HTTP请求的URL进行路由 同一个TCP端口进来的流量可以根据URL路由到Cluster中的不同服务，如下图所示： 图片 - 按HTTP请求的ULR进行路由 按HTTP请求的Host进行路由 同一个IP进来的流量可以根据HTTP请求的Host路由到Cluster中的不同服务，如下图所示： 图片 - 按HTTP请求的Host进行路由 Ingress 规则定义了对七层网关的要求，包括URL分发规则，基于不同域名的虚拟主机，SSL证书等。Kubernetes使用Ingress Controller 来监控Ingress规则，并通过一个七层网关来实现这些要求，一般可以使用Nginx，HAProxy，Envoy等。 虽然Ingress Controller通过七层网关为后端的多个Service提供了统一的入口，但由于其部署在集群中，因此并不能直接对外提供服务。实际上Ingress需要配合NodePort和LoadBalancer才能提供对外的流量入口，如下图所示： 图片 - 采用Ingress, NodePortal和LoadBalancer提供外部流量入口的拓扑结构 上图描述了如何采用Ingress配合NodePort和Load Balancer为集群提供外部流量入口，从该拓扑图中可以看到该架构的伸缩性非常好，在NodePort，Ingress，Pod等不同的接入层面都可以对系统进行水平扩展，以应对不同的外部流量要求。 上图只展示了逻辑架构，下面的图展示了具体的实现原理： 图片 - 采用Ingress, NodePortal和LoadBalancer提供外部流量入口的实现原理 流量从外部网络到达Pod的完整路径如下： 外部请求先通过四层Load Balancer进入内部网络 Load Balancer将流量分发到后端多个主机节点上的NodePort (userspace转发) 请求从NodePort进入到Ingress Controller (iptabes规则，Ingress Controller本身是一个NodePort类型的Service) Ingress Controller根据Ingress rule进行七层分发，根据HTTP的URL和Host将请求分发给不同的Service (userspace转发) Service将请求最终导入到后端提供服务的Pod中 (iptabes规则) 从前面的介绍可以看到，K8S Ingress提供了一个基础的七层网关功能的抽象定义，其作用是对外提供一个七层服务的统一入口，并根据URL/HOST将请求路由到集群内部不同的服务上。 如何为服务网格选择入口网关？ 在Istio服务网格中，通过为每个Service部署一个sidecar代理，Istio接管了Service之间的请求流量。控制面可以对网格中的所有sidecar代理进行统一配置，实现了对网格内部流量的路由控制，从而可以实现灰度发布，流量镜像，故障注入等服务管控功能。但是，Istio并没有为入口网关提供一个较为完善的解决方案。 K8s Ingress 在0.8版本以前，Istio缺省采用K8s Ingress来作为Service Mesh的流量入口。K8s Ingress统一了应用的流量入口，但存在两个问题： K8s Ingress是独立在Istio体系之外的，需要单独采用Ingress rule进行配置，导致系统入口和内部存在两套互相独立的路由规则配置，运维和管理较为复杂。 K8s Ingress rule的功能较弱，不能在入口处实现和网格内部类似的路由规则，也不具备网格sidecar的其它能力，导致难以从整体上为应用系统实现灰度发布、分布式跟踪等服务管控功能。 图片 - 采用Kubernetes Ingress作为服务网格的流量入口 Istio Gateway Istio社区意识到了Ingress和Mesh内部配置割裂的问题，因此从0.8版本开始，社区采用了 Gateway 资源代替K8s Ingress来表示流量入口。 Istio Gateway资源本身只能配置L4-L6的功能，例如暴露的端口，TLS设置等；但Gateway可以和绑定一个VirtualService，在VirtualService 中可以配置七层路由规则，这些七层路由规则包括根据按照服务版本对请求进行导流，故障注入，HTTP重定向，HTTP重写等所有Mesh内部支持的路由规则。 Gateway和VirtualService用于表示Istio Ingress的配置模型，Istio Ingress的缺省实现则采用了和Sidecar相同的Envoy proxy。 通过该方式，Istio控制面用一致的配置模型同时控制了入口网关和内部的sidecar代理。这些配置包括路由规则，策略检查、Telementry收集以及其他服务管控功能。 图片 - 采用 Istio Ingress Gateway作为服务网格的流量入口 应用对API Gateway的需求 采用Gateway和VirtualService实现的Istio Ingress Gateway提供了网络入口处的基础通信功能，包括可靠的通信和灵活的路由规则。但对于一个服务化应用来说，网络入口除了基础的通讯功能之外，还有一些其他的应用层功能需求，例如： 第三方系统对API的访问控制 用户对系统的访问控制 修改请求/返回数据 服务API的生命周期管理 服务访问的SLA、限流及计费 …. 图片 - Kubernetes ingress, Istio gateway and API gateway的功能对比 API Gateway需求中很大一部分需要根据不同的应用系统进行定制，目前看来暂时不大可能被纳入K8s Ingress或者Istio Gateway的规范之中。为了满足这些需求，涌现出了各类不同的k8s Ingress Controller以及Istio Ingress Gateway实现，包括Ambassador ，Kong, Traefik,Solo等。 这些网关产品在实现在提供基础的K8s Ingress能力的同时，提供了强大的API Gateway功能，但由于缺少统一的标准，这些扩展实现之间相互之间并不兼容。而且遗憾的是，目前这些Ingress controller都还没有正式提供和Istio 控制面集成的能力。 备注： Ambassador将对Istio路由规则的支持纳入了Roadmap https://www.getambassador.io/user-guide/with-istio/ Istio声称支持Istio-Based Route Rule Discovery (尚处于实验阶段) https://gloo.solo.io/introduction/architecture/ 采用API Gateway + Sidecar Proxy作为服务网格的流量入口 在目前难以找到一个同时具备API Gateway和Isito Ingress能力的网关的情况下，一个可行的方案是使用API Gateway和Sidecar Proxy一起为服务网格提供外部流量入口。 由于API Gateway已经具备七层网关的功能，Mesh Ingress中的Sidecar只需要提供VirtualService资源的路由能力，并不需要提供Gateway资源的网关能力，因此采用Sidecar Proxy即可。网络入口处的Sidecar Proxy和网格内部应用Pod中Sidecar Proxy的唯一一点区别是：该Sidecar只接管API Gateway向Mesh内部的流量，并不接管外部流向API Gateway的流量；而应用Pod中的Sidecar需要接管进入应用的所有流量。 图片 - 采用API Gateway + Sidecar Proxy为服务网格提供流量入口 备注：在实际部署时，API Gateway前端需要采用NodePort和LoadBalancer提供外部流量入口。为了突出主题，对上图进行了简化，没有画出NodePort和LoadBalancer。 采用API Gateway和Sidecar Proxy一起作为服务网格的流量入口，既能够通过对网关进行定制开发满足产品对API网关的各种需求，又可以在网络入口处利用服务网格提供的灵活的路由能力和分布式跟踪，策略等管控功能，是服务网格产品入口网关的一个理想方案。 性能方面的考虑：从上图可以看到，采用该方案后，外部请求的处理流程在入口处增加了Sidecar Proxy这一跳，因此该方式会带来少量的性能损失，但该损失是完全可以接受的。 对于请求时延而言，在服务网格中，一个外部请求本来就要经过较多的代理和应用进程的处理，在Ingress处增加一个代理对整体的时延影响基本忽略不计，而且对于绝大多数应用来说，网络转发所占的时间比例本来就很小，99%的耗时都在业务逻辑。如果系统对于增加的该时延非常敏感，则建议重新考虑该系统是否需要采用微服务架构和服务网格。 对于吞吐量而言，如果入口处的网络吞吐量存在瓶颈，则可以通过对API Gateway + Sidecar Proxy组成的Ingress整体进行水平扩展，来对入口流量进行负荷分担，以提高网格入口的网络吞吐量。 参考 Virtual IPs and Service Proxie - kubernetes.io 如何从外部访问Kubernetes集群中的应用？ - zhaohuabing.com The obstacles to put Istio into production and how we solve them - kubernetes.io Kubernetes NodePort vs LoadBalancer vs Ingress? When should I use what? - medium.com 点击关注【ServiceMesher】 微信公众号回复【加群】加入社区Copyright © servicemesher.com 2018-2019 all right reserved，powered by Gitbook Updated at 2019-03-30 22:15:08 "},"appendix/service-mesh-landscape.html":{"url":"appendix/service-mesh-landscape.html","title":"服务网格全景图","summary":"本文描绘了服务网格的全景图。","keywords":"","body":"服务网格全景图 Service Mesh 的概念于 2016 年诞生至今仍在蓬勃发展，下面是由 ServiceMesher 社区维护的Service Mesh 列表，如您发现该列表中有所遗漏欢迎到 servicemesher/awesome-servicemesh 上提交 PR。 amalgam8 - 用于异构微服务的基于版本的路由网格 ambassador - 开源的基于 Envoy proxy 构建的用于微服务的 Kubernetes 原生 API 网关 https://www.getambassador.io aspen-mesh - 隶属于 F5 公司开发的 Service Mesh consul - Consul 一种分布式、高可用的数据中心感知解决方案，用于跨动态分布式基础架构连接和配置应用程序。https://www.consul.io/ dubbo - Apache Dubbo™ (incubating)是一款高性能Java RPC框架。http://dubbo.incubator.apache.org envoy - C++ 前端/服务代理 https://www.envoyproxy.io istio - 用于连接、保护、控制和观测服务。 kong - 云原生 API 网关 https://konghq.com/install linkerd - 云原生应用的开源 Service Mesh https://linkerd.io mesher - 华为开源的轻量级基于 go chassis 的 Service Mesh。 nginmesh - 基于 Nginx 的 Service Mesh nginx-unit - NGINX Unit is a new, lightweight, open source application server built to meet the demands of today’s dynamic and distributed applications. servicecomb - ServiceComb 是华为开源的微服务框架，提供便捷的在云中开发和部署应用的方式。 sofa-mesh - SOFAMesh 是蚂蚁金服开源的基于 Istio 的大规模服务网格解决方案。 http://www.sofastack.tech/ sofa-mosn - SOFAMosn 是由蚂蚁金服开源的一个模块化可观测的智能网络，可作为 sidecar 部署在 Service Mesh 中。http://www.sofastack.tech tars - Tars 是腾讯开源的基于名称服务的高性能 RPC 框架。使用 tars 协议并提供半自动化运维平台。 Orange - Orange 是一个简洁 高性能的云原生网关 点击关注【ServiceMesher】 微信公众号回复【加群】加入社区Copyright © servicemesher.com 2018-2019 all right reserved，powered by Gitbook Updated at 2019-04-03 20:13:45 "}}